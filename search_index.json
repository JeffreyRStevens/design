[["index.html", "Tidyverse design guide Welcome", " Tidyverse design guide Tidyverse team Welcome The goal of this book is to help you write better R code. It has four main components: Design problems which lead to suboptimal outcomes. Useful patterns that help solve common problems. Key principles that help you balance conflicting patterns. Selected case studies that help you see how all the pieces fit together with real code. It is used by the tidyverse team to promote consistency across packages in the core tidyverse. But you can also use it to guide the design of your functions and packages to create interfaces that feel “tidy”, and fit in natural with other code that uses tidyverse idioms. It is a complement to http://style.tidyverse.org, which focusses on low-level code formatting. This book will be under heavy development for quite some time; currently we are loosely aiming for completion in 2020. You’ll find many chapters contain disjointed text that mostly serve as placeholders for the authors, and I do not recommend attempting to systematically read the book at this time. "],["structure.html", "1 Structure 1.1 Implementation 1.2 Interface", " 1 Structure 1.1 Implementation 1.2 Interface The interface of a function describes how it interfaces with the world, independent of its internal implementation. 1.2.1 Inputs Chapter 6: All inputs to a function should be explicit arguments. Avoid functions that suprise the user by returning different results when the inputs look the same. Chapter 7: Required arguments should come before optional arguments. Chapter 8: Make arguments as orthogonal as possible. Avoid complex interdependencies. 1.2.1.1 Default values Chapter 10: the absence of a default value should indicate that an argument is required; the presence of a default value should indicate that an argument is optional. Chapter 12: If a details argument can take one of a fixed set of possible strings, record them in the default value and use match.arg() or rlang::arg_match() inside the function. Chapter 13: Default values should return the same answer when set directly. Chapter 14: Default values should be short. If you have a complex calculation, either use NULL or an exported function. Chapter 15: If a default value is particularly important, as has non-trivial calculation, let the user know what it is. Chapter 16: Use getOption() to allow the user to set default values. 1.2.1.2 Dots Chapter 18: ... should be placed between the data and details arguments. Chapter 19: don’t use ... just to save the user from typing c() (unless the function is purely for data structure creation). Chapter 20: carefully consider if all other arguments need a . prefix in order to reduce the chance of spurious matches. Chapter 21: when using ... with S3 methods, or passing on to other functions that silently ignore mismatches, check that all inputs in ... are evaluated. 1.2.2 Output Chapter 26: functions called primarily for their side-effects should invisibly return a useful value. 1.2.3 Errors Chapter 27: don’t display the call when generating an error message. Chapter 28: if the same error is generated by multiple functions, you should extract it out into its own function, an error constructor. 1.2.4 Side-effects Chapter 30: mixing side-effects and computation in a single function makes for functions that are hard to reason about and hard to program with. Chapter 31: if a function has side-effects, it should be constrained to lie at or beneath the current scope. "],["unifying-principles.html", "2 Unifying principles 2.1 Human centered 2.2 Consistent 2.3 Composable 2.4 Inclusive", " 2 Unifying principles The tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate the conversation that a human has with a dataset, and we want to help dig a “pit of success” where the least-effort path trends towards a positive outcome. The primary tool to dig the pit is API design: by carefully considering the external interface to a function, we can help guide the user towards success. But it’s also necessary to have some high level principles that guide how we think broadly about APIs, principles that we can use to “break ties” when other factors are balanced. The tidyverse has four guiding principles: It is human centered, i.e. the tidyverse is designed specifically to support the activities of a human data analyst. It is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible. It is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution. It is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them. These guiding principles are aspirational; they’re not always fully realised in current tidyverse packages, but we strive to make them so. Related work These principles are inspired by writings about the design of other systems: such as: The Unix philsophy The Zen of Python Design Principles Behind Smalltalk 2.1 Human centered Programs must be written for people to read, and only incidentally for machines to execute. — Hal Abelson Programming is a task performed by humans. To create effective programming tools we must explicitly recognise and acknowledge the role played by cognitive psychology. This is particularly important for R, because it’s a language that’s used primarily by non-programmers, and we want to make it as easy as possible for first-time and end-user programmers to learn the tidyverse. A particularly useful tool from cognitive psychology is “cognitive load theory”1: we have a limited working memory, and anything we can do to reduce extraneous cognitive load helps the learner and user of the tidyverse. This motivates the next two principles: By being consistent you only need to learn and internalise one expression of an idea, and then you can apply that many times. By being composable you can break down complex problems into bite sized pieces that you can easily hold in your head. Idea of “chunking” is important. Some setup cost to learn a new chunk, but once you’ve internalised it, it only takes up one spot in your working memory. In some sense the goal of the tidyverse is to discover the minimal set of chunks needed to do data science and have some sense of the priority of the remainder. Other useful ideas come from design. One particularly powerful idea is that of “affordance”: the exterior of a tool should suggest how to use it. We want to avoid “Norman doors” where the exterior clues and cues point you in the wrong direction. This principle is deeply connected to our beliefs about performance. Most importantly performance of code depends not only on how long it takes to run, but also how long it takes to write and read. Human brains are typically slower than computers, so this means we spend a lot of time thinking about how to create intuitve interfaces, focussing on writing and reading speed. Intuitive interfaces sometimes are at odds with running speed, because writing the fastest code for a problem often requires designing the interface for performance rather than usability. Generally, we optimise first for humans, then use profiling to discover bottlenecks that cause friction in data analysis. Once we have identified an important bottleneck, then performance becomes a priority and we rewrite the existing code. Generally, we’ll attempt to preserve the existing interface, only changing it when the performance implications are significant. 2.2 Consistent A system should be built with a minimum set of unchangeable parts; those parts should be as general as possible; and all parts of the system should be held in a uniform framework. — Daniel H. H. Ingalls The most important API principle of the tidyverse is to be consistent. We want to find the smallest possible set of key ideas and use them again and again. This is important because it makes the tidyverse easier to learn and remember. (Another framing of this principle is Less Volume, More Creativity, which comes from Mike McCarthy, the head coach of the Green Bay Packers, and popularised in Statistics Education by Randall Pruim) This is related to one of my favourite saying from the Python community: There should be one—and preferably only one—obvious way to do it. — Zen of Python The tidyverse aspires to put this philosophy into practice. However, because the tidyverse is embedded within the larger R ecosystem, applying this principle never needs to be 100% comprehensive. If you can’t solve a problem from within the tidyverse, you can always step outside and do so with base R or another package. This also means that we don’t have to rush to cover every possible use case; we can take our time to develop the best new solutions. The principle of consistency reveals itself in two primary ways: in function APIs and in data structures. The API of a function defines its external interface (independent of its internal implementation). Having consistent APIs means that each time you learn a function, learning the next function is a little easier; once you’ve mastered one package, mastering the next is easier. There are two ways that we make functions consistent that are so important that they’re explicitly pull out as high-level principles below: Functions should be composable: each individual function should tackle one well contained problem, and you solve complex real-world problems by composing many individual functions. Overall, the API should feel “functional”, which is a technical term for the programming paradigm favoured by the tidyverse But consistency also applies to data structures: we want to ensure we use the same data structures again and again and again. Principally, we expect data to be stored in tidy data frames or tibbles. This means that tools for converting other formats can be centralised in one place, and that packages development is simplified by assuming that data is already in a standard format. Valuing consistency is a trade-off, and we explicitly value it over performance. There are cases where a different data structure or a different interface might make a solution simpler to express or much faster. However, one-off solutions create a much higher cognitive load. 2.3 Composable No matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system. — Hal Abelson A powerful strategy for solving complex problems is to combine many simple pieces. Each piece should be easily understood in isolation, and have a standard way of combining with other pieces. Within the tidyverse, we prefer to compose functions using a single tool: the pipe, %&gt;%. There are two notable exceptions to this principle: ggplot2 composes graphical elements with +, and httr composes requests primarily through .... These are not bad techniques in isolation, and they are well suited to the domains in which they are used, but the disadvantages of inconsistency outweigh any local advantages. For smaller domains, this means carefully designing functions so that the inputs and outputs align (e.g. the output from stringr::str_locate() can easily be fed into str_sub()). For middling domains, this means drawing many feature matrices and ensuring that they are dense (e.g. consider the map family in purrr). For larger domains, this means carefully thinking about algebras and grammars, identifying the atoms of a problem and the ways in which they might be composed to solve bigger problems. We decompose large problems into smaller, more tractable ones by creating and combining functions that transform data rather than by creating objects whose state changes over time. Other techniques that tend to facilitate composability: Functions are data: this leads some of the most impactful techniques for functional programming, which allow you to reduce code duplication. Immutable objects. Enforces independence between components. Partition side-effects. Type-stable. 2.4 Inclusive We value not just the interface between the human and the computer, but also the interface between humans. We want the tidyverse to be a diverse, inclusive, and welcoming community. We develop educational materials that are accessible to people with many different skill levels. We prefer explicit codes of conduct. We create safe and friendly communities. We believe that kindness should be a core value of communities. We think about how we can help others who are not like us (they may be visually impaired or may not speak English). We also appreciate the paradox of tolerance: the only people that we do not welcome are the intolerant. A good practical introduction is Cognitive load theory in practice (PDF).↩︎ "],["names-attribute.html", "3 Names attribute 3.1 Coverage in tidyverse style guide 3.2 The names attribute of an object 3.3 Minimal, unique, universal 3.4 Minimal names 3.5 Unique names 3.6 Universal names 3.7 Messaging user about name repair", " 3 Names attribute 3.1 Coverage in tidyverse style guide Existing name-related topics in http://style.tidyverse.org File names Object names Argument names Function names 3.2 The names attribute of an object Here we address how to manage the names attribute of an object. Our initial thinking was motivated by how to handle the column or variable names of a tibble, but is evolving into a name-handling strategy for vectors, in general. The name repair described below is exposed to users via the .name_repair argument of tibble::tibble(), tibble::as_tibble(), readxl::read_excel(), and, eventually other packages in the tidyverse. This work was initiated in the tibble package, but is migrating to the vctrs package. Name repair was first introduced in tibble v2.0.0 and this write-up is being rendered with tibble v3.1.7 and vctrs v0.4.1. These are the kind of names we’re talking about: ## variable names names(iris) #&gt; [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; names(ChickWeight) #&gt; [1] &quot;weight&quot; &quot;Time&quot; &quot;Chick&quot; &quot;Diet&quot; ## names along a vector names(euro) #&gt; [1] &quot;ATS&quot; &quot;BEF&quot; &quot;DEM&quot; &quot;ESP&quot; &quot;FIM&quot; &quot;FRF&quot; &quot;IEP&quot; &quot;ITL&quot; &quot;LUF&quot; &quot;NLG&quot; &quot;PTE&quot; 3.3 Minimal, unique, universal We identify three nested levels of naminess that are practically useful: Minimal: The names attribute is not NULL. The name of an unnamed element is \"\" (the empty string) and never NA. Unique: No element of names appears more than once. A couple specific names are also forbidden in unique names, such as \"\" (the empty string). All columns can be accessed by name via df[[\"name\"]] and, more generally, by quoting with backticks: df$`name`, subset(df, select = `name`), and dplyr::select(df, `name`). Universal: The names are unique and syntactic. Names work everywhere, without quoting: df$name and lm(name1 ~ name2, data = df) and dplyr::select(df, name) all work. Below we give more details and describe implementation. 3.4 Minimal names Minimal names exist. The names attribute is not NULL. The name of an unnamed element is \"\" (the empty string) and never NA. Consider an unnamed vector, i.e. it has names attribute of NULL. x &lt;- letters[1:3] names(x) #&gt; NULL Repair the names to make them minimal. # TODO: Figure out where `set_minimal_names()` ended up when name repair moved # from tibble to vctrs, then update this chunk. Use rlang in the meantime. # x &lt;- tibble:::set_minimal_names(x) x &lt;- rlang::set_names(x, rlang::names2(x)) names(x) Minimal names appear to be a useful baseline requirement, if the names attribute of an object is going to be actively managed. Why? General name handling and repair can be implemented more simply if the baseline strategy guarantees that names(x) returns a character vector of the correct length with no NAs. This is also a reasonable interpretation of base R’s intent for named vectors, based on the docs for names(), although base R’s implementation/enforcement of this is uneven. From ?names: The name \"\" is special: it is used to indicate that there is no name associated with an element of a (atomic or generic) vector. Subscripting by \"\" will match nothing (not even elements which have no name). A name can be character NA, but such a name will never be matched and is likely to lead to confusion. tbl_df objects created by tibble::tibble() and tibble::as_tibble() have variable names that are minimal, at the very least. The function rlang::names2() returns the names of an object, after making them minimal. x &lt;- letters[1:3] names(x) #&gt; NULL rlang::names2(x) #&gt; [1] &quot;&quot; &quot;&quot; &quot;&quot; 3.5 Unique names Unique names meet the requirements for minimal and have no duplicates. In the tidyverse, we go further and repair a few specific names: \"\" (the empty string), ... (R’s ellipsis or “dots” construct), and ..j where j is a number. They are basically all treated like \"\", which is always repaired. Example of unique-ified names: original unique-ified &quot;&quot; ...1 x x...2 &quot;&quot; ...3 ... ...4 y y x x...6 This augmented definition of unique has a specific motivation: it ensures that each element can be identified by name, at least when protected by backtick quotes. Literally, all of these work: df[[&quot;name&quot;]] df$`name` with(df, `name`) subset(df, select = `name`) dplyr::select(df, `name`) This has practical significance for variable names inside a data frame, because so many workflows rely on indexing by name. Note that uniqueness refers implicitly to a vector of names. Let’s explore a few edge cases: A single dot followed by a number, .j, does not need repair. df &lt;- tibble(`.1` = &quot;ok&quot;) df$`.1` #&gt; [1] &quot;ok&quot; subset(df, select = `.1`) #&gt; # A tibble: 1 × 1 #&gt; `.1` #&gt; &lt;chr&gt; #&gt; 1 ok dplyr::select(df, `.1`) #&gt; # A tibble: 1 × 1 #&gt; `.1` #&gt; &lt;chr&gt; #&gt; 1 ok Two dots followed by a number, ..j, does need repair. The same goes for three dots, ..., the ellipsis or “dots” construct. These can’t function as names, even if quoted with backticks, so they have to be repaired. df &lt;- tibble(`..1` = &quot;not ok&quot;) #&gt; Error: #&gt; ! Column 1 must not have names of the form ... or ..j. #&gt; Use .name_repair to specify repair. #&gt; Caused by error in `repaired_names()`: #&gt; ! Names can&#39;t be of the form `...` or `..j`. #&gt; ✖ These names are invalid: #&gt; * &quot;..1&quot; at location 1. with(df, `..1`) #&gt; Error in eval(substitute(expr), data, enclos = parent.frame()): ..1 used in an incorrect context, no ... to look in dplyr::select(df, `..1`) #&gt; Error in `dplyr::select()`: #&gt; ! &#39;...&#39; used in an incorrect context df &lt;- tibble(`...` = &quot;not ok&quot;) #&gt; Error: #&gt; ! Column 1 must not have names of the form ... or ..j. #&gt; Use .name_repair to specify repair. #&gt; Caused by error in `repaired_names()`: #&gt; ! Names can&#39;t be of the form `...` or `..j`. #&gt; ✖ These names are invalid: #&gt; * &quot;...&quot; at location 1. subset(df, select = `...`) #&gt; Error in eval(expr, envir, enclos): &#39;...&#39; used in an incorrect context dplyr::select(df, `...`) #&gt; Error in eval(expr, envir, enclos): &#39;...&#39; used in an incorrect context Both are repaired as if they were \"\". 3.5.1 Making names unique There are many ways to make names unique. We append a suffix of the form ...j to any name that is a duplicate or \"\" or ..., where j is the position. Why? An absolute position j is more helpful than numbering within the elements that share a name. Context: troubleshooting data import with lots of columns and dysfunctional names. We hypothesize that it’s better have a “level playing field” when repairing names, i.e. if foo appears twice, both instances get repaired, not just the second occurrence. The unique level of naminess is regarded as normative for a tibble and a user must expressly request a tibble with names that violate this (but that is possible). Base R’s function for this is make.unique(). We revisit the example above, comparing the tidyverse strategy for making names unique vs. what make.unique() does. Original Unique names Result of names (tidyverse) make.unique() &quot;&quot; ...1 &quot;&quot; x x...2 x &quot;&quot; ...3 .1 ... ...4 ... y y y x x...6 x.1 3.5.2 Roundtrips When unique-ifying names, we assume that the input names have been repaired by the same strategy, i.e. that we are consuming dogfood. Therefore, pre-existing suffixes of the form ...j are stripped, prior to (re-)constructing the suffixes. If this interacts poorly with your names, you need to take control of name repair. Example of re-unique-ified names: original unique-ified ...5 ...1 x x...2 x...3 x...3 &quot;&quot; ...4 x...1...5 x...5 JB: it is conceivable that this should be under the control of an argument, e.g. dogfood = TRUE, in the (currently unexported) function that does this 3.5.3 When is minimal better than unique? Why would you ever want to import a tibble and enforce only minimal names, instead of unique? Sometimes the first row of a data source – allegedly variable names – actually contains data and the resulting tibble will be reshaped with, e.g., tidyr::gather(). In this case, it is better to not munge the names at import. This is a common special case of the “data stored in names” phenomenon. In general, you may want to tolerate minimal names when the dysfunctional names are just an awkward phase that an object is passing through and a more definitive solution is applied downstream. 3.5.4 Ugly, with a purpose You might say that names like x...5 are ugly and you would be right. We’re calling this a feature, not a bug! Names that have been automatically unique-ified by the tidyverse should catch the eye and give the user strong encouragement to take charge of the situation. 3.5.5 Why so many dots? The suffix of ...j, with 3 leading dots, is the result of jointly satisfying multiple requirements. It is important to anticipate a missing name, where the suffix becomes the entire name. We have elected to make the suffix a syntactic name (more below), because non-syntactic names are a frequent cause of unexpected friction for users. This means the suffix can’t be j, .j, or ..j, because all are non-syntactic. It must be ...j. 3.5.6 Why dot(s) in the first place? The underscore _ was also considered when choosing the suffix strategy, but was rejected. Why? Because syntactic names can’t start with an underscore and we want the suffix itself to be syntactic. Also, the dot . is already used by base R’s make.names() to replace invalid characters. It seems simpler and, therefore, better to use the same character, in the same way, as much as possible in name repair. We use the dot ., we put it at the front, as many times as necessary. 3.6 Universal names Universal names are unique, in the sense described above, and syntactic, in the normal R sense. Universal names are appealing because they play nicely with base R and tidyverse functions that accept unquoted variable names. 3.6.1 Syntactic names A syntactic name in R: Consists of letters, numbers, and the dot . or underscore _ characters. Starts with a letter or starts with a dot . followed by anything but a number. Is not a reserved word, such as if or function or TRUE. Is not ..., R’s special ellipsis or “dots” construct. Is not of the form ..j, where j is a number. See R’s documentation for Reserved words and Quotes, specifically the section on names and identifiers. A syntactic name can be used “as is” in code. For example, it does not require quoting in order to work with non-standard evaluation, such as list indexing via $, in a formula, or in packages like dplyr and ggplot2. ## a syntactic name doesn&#39;t require quoting x &lt;- tibble::tibble(.else = &quot;else?!&quot;) x$.else #&gt; [1] &quot;else?!&quot; dplyr::select(x, .else) #&gt; # A tibble: 1 × 1 #&gt; .else #&gt; &lt;chr&gt; #&gt; 1 else?! ## use a non-syntactic name x &lt;- tibble::tibble(`else` = &quot;else?!&quot;) ## this code does not parse # x$else # dplyr::select(x, else) ## a non-syntacitic name requires quoting x$`else` #&gt; [1] &quot;else?!&quot; dplyr::select(x, `else`) #&gt; # A tibble: 1 × 1 #&gt; `else` #&gt; &lt;chr&gt; #&gt; 1 else?! Note that being syntactic is a property of an individual name. 3.6.2 Making an individual name syntactic There are many ways to fix a non-syntactic name. Here’s how our logic compares to base::make.names() for a single name: Same: Definition of what is syntactically valid. Claim: If syn_name is a name that we have made syntactic, then syn_name == make.names(syn_name). If you find a counterexample, tell us! Same: An invalid character is replaced with a dot .. Different: We always fix a name by prepending a dot .. base::make.names() sometimes prefixes with X and at other times appends a dot .. This means we turn ... into .... and ..j into ...j, where j is a number. base::make.names() does not modify ... or ..j, which could be regarded as a bug (?). Different: We treat NA and \"\" the same: both become .. This is because we first make names minimal. base::make.names() turns NA into \"NA.\" and \"\" into \"X\". Examples of the tidyverse approach to making individual names syntactic versus base::make.names(): Original Syntactic name Result of name (tidyverse) make.names() &quot;&quot; . X NA . NA. (y) .y. X.y. _z ._z X_z .2fa ..2fa X.2fa FALSE .FALSE FALSE. ... .... ... ..3 ...3 ..3 Currently implemented in the unexported function tibble:::make_syntactic(). 3.6.3 Why universal? Now we can state the motivation for universal names, which have the group-wise property of being unique and the element-wise property of being syntactic. In practice, if you want syntactic names, you probably also want them to be unique. You need both in order to refer to individual elements easily, without ambiguity and without quoting. Universal names can be requested in the tidyverse via .name_repair = \"universal\", in functions that expose name repair. 3.6.4 Making names universal Universal names are implemented as a variation on unique names. Basically, suffixes are stripped and ... is replaced with \"\". These draft names are transformed with tibble:::make_syntactic() (this step is omitted for unique names). Then ...j suffixes are appended as necessary. Note that suffix stripping and the substitution of \"\" for ... happens before the draft names are made syntactic. So, although tibble:::make_syntactic turns ... into ...., universal or unique name repair will turn ... into something of the form ...j. 3.7 Messaging user about name repair Name repair should be communicated to the user. Here’s how tibble messages: x &lt;- tibble::tibble( x = 1, x = 2, `a1:` = 3, `_x_y}` = 4, .name_repair = &quot;universal&quot; ) #&gt; New names: #&gt; • `x` -&gt; `x...1` #&gt; • `x` -&gt; `x...2` #&gt; • `a1:` -&gt; `a1.` #&gt; • `_x_y}` -&gt; `._x_y.` "],["call-data-details.html", "4 Name details arguments 4.1 What are data and details arguments? 4.2 What’s the pattern? 4.3 Why is this useful? 4.4 What are the exceptions?", " 4 Name details arguments 4.1 What are data and details arguments? The arguments to a function typically fall into two broad sets: one set supplies the data to compute on, and the other supplies arguments that control the details of the computation. For example: In log(), the data is x, and the detail is the base of the logarithm. In mean(), the data is x, and the details are how much data to trim from the ends (trim) and how to handle missing values (na.rm). In t.test(), the data are x and y, and the details of the test are specified by the alternative, mu, paired, var.equal, and conf.level arguments. Typically, data arguments don’t have default values, and work with vectors or data frames, while details arguments have defaults, and take single values (like TRUE or FALSE, or a single string that specifies a method). 4.2 What’s the pattern? When calling a function, data arguments come first, specified by position, followed by details arguments specified by name. y &lt;- c(1:10, NA) # Good mean(y, na.rm = TRUE) #&gt; [1] 5.5 # Bad mean(x = y, , TRUE) #&gt; [1] 5.5 mean(, TRUE, x = y) #&gt; [1] 5.5 Never use partial matching, which allows you to refer to an argument by a unique prefix, e.g. mean(x, n = TRUE). Partial matching was useful in the early days of R because when you were doing a quick and dirty interactive analysis you could save a little time by shortening argument names. However, today, most R editing environments support autocomplete so partial matching only saves you a single keystroke, and it makes code substantially harder to read. You can make R give you are warning that you’re using a partially named argument with a special option. Call usethis::use_partial_warnings() to make this the default for all R sessions. options(warnPartialMatchArgs = TRUE) mean(x = 1:10, n = FALSE) #&gt; Warning in mean.default(x = 1:10, n = FALSE): partial argument match of &#39;n&#39; to #&gt; &#39;na.rm&#39; #&gt; [1] 5.5 4.3 Why is this useful? I think it’s reasonable to assume that the reader knows what a function does then they know what the data arguments are (and their order), and repeating their names just takes up space without aiding communication. This then leads naturally to %&gt;% where you don’t specify the name of the first argument either (since it comes from the left-hand side of %&gt;%.) However, I don’t think it’s reasonable to expect that people will remember the order of the details arguments. For example, I don’t think that most people know that the second argument to mean() is trim, even though mean() is an extremely commonly used function. Spelling the names out in 4.4 What are the exceptions? I think the main exception to this rule is when you are teaching a function for the first time. It makes sense to emphasis the names of the data arguments to help people understand exactly what’s going on. For example, in R for data science when we introduce ggplot2 we write code like: ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() At the end of the chapter, we assume that the reader is familiar with the basic structure and so the rest of the book uses the style recommended here: ggplot(mpg, aes(displ, hwy)) + geom_point() "],["function-names.html", "5 Function names 5.1 Nouns vs verbs 5.2 Function families 5.3 Length 5.4 Conflicts 5.5 Techniques 5.6 Other good advice", " 5 Function names Follow the style guide (i.e. use snake_cake). 5.1 Nouns vs verbs In general, prefer verbs. Use imperative mood: mutate() not mutated(), mutates(), or mutating(); do() not did(), does(), doing(), hide() not hid(), hides(), or hiding(). Exception: noun-y interfaces where you’re building up a complex object like ggplot2 or recipes (verb-y interface in ggvis was a mistake). Nouns should be singular (geom_point() not geom_points()), simply because the plurisation rules in English are complex. 5.2 Function families Use prefixes to group functions together based on common input or common purpose. Prefixes are better than suffixes because of auto-complete. Examples: ggplot2, purrr. Counter example: shiny. Not sure about common prefixes for a package. Works well for stringr (esp. with stringi), forcats, xml2, and rvest. But there’s only a limited number of short prefixes and I think it would break down if every package did it. Use suffixes for variations on a theme (e.g. map_int(), map_lgl(), map_dbl(); str_locate(), str_locate_all().) Strive for thematic unity in related functions. Can you make related fuctions rhyme? Or have the same number of letters? Or similar background (i.e. all Germanic origins vs. French). 5.3 Length Err on the side of too long rather than too short (reading is generally more important than writing). Autocomplete will mostly take care of the nuisance and you can always shorten later if you come up with a better name. (But hard to make long later, and you may take up a good word that is a lot of work to reclaim later). Length of name should be inversely proportional to frequency of usage. Reserve very short words for functions that are likely to be used very frequently. 5.4 Conflicts You can’t expect to avoid conflicts with every existing CRAN package, but you should strive to avoid conflicts with “nearby” packages (i.e. packages that are commonly used with your package). 5.5 Techniques Thesaurus List of common verbs Rhyming dictionary 5.6 Other good advice I Shall Call It.. SomethingManager The Poetry of Function Naming "],["args-hidden.html", "6 Avoid hidden arguments 6.1 What’s the problem? 6.2 What are some examples? 6.3 Why is it important? 6.4 How can I remediate the problem?", " 6 Avoid hidden arguments 6.1 What’s the problem? Functions are easier to understand if the results depend only on the values of the inputs. If a function returns surprisingly different results with the same inputs, then we say it has hidden arguments. Hidden arguments make code harder to reason about, because to correctly predict the output you also need to know some other state. Related: This pattern is about surprising inputs; Spooky action is about suprising outputs. 6.2 What are some examples? One common source of hidden arguments is the use of global options. These can be useful to control display but, as discussed in Chapter 16), should not affect computation: The result of data.frame(x = \"a\")$x depends on the value of the global stringsAsFactors option: if it’s TRUE (the default) you get a factor; if it’s false, you get a character vector. lm()’s handling of missing values depends on the global option of na.action. The default is na.omit which drops the missing values prior to fitting the model (which is inconvenient because then the results of predict() don’t line up with the input data. modelr::na.warn() provides an approach more in line with other base behaviours: it drops missing values with a warning.) Another common source of hidden inputs is the system locale: strptime() relies on the names of weekdays and months in the current locale. That means strptime(\"1 Jan 2020\", \"%d %b %Y\") will work on computers with an English locale, and fail elsewhere. This is particularly troublesome for Europeans who frequently have colleagues who speak a different language. as.POSIXct() depends on the current timezone. The following code returns different underlying times when run on different computers: as.POSIXct(&quot;2020-01-01 09:00&quot;) #&gt; [1] &quot;2020-01-01 09:00:00 UTC&quot; toupper() and tolower() depend on the current locale. It is faily uncommon for this to cause problems because most languages either use their own character set, or use the same rules for capitalisation as English. However, this behaviour did cause a bug in ggplot2 because internally it takes geom = \"identity\" and turns it into GeomIdentity to find the object that actually does computation. In Turkish, however, the upper case version of i is İ, and Geomİdentity does not exist. This meant that for some time ggplot2 did not work on Turkish computers. library(stringr) str_to_upper(&quot;i&quot;) #&gt; [1] &quot;I&quot; str_to_upper(&quot;i&quot;, locale = &quot;tr&quot;) #&gt; [1] &quot;İ&quot; For similar reasons, sort() and order() rely on the lexicographic order defined by the current locale. factor() uses order(), so the results from factor depend implicitly on the current locale. (This is not an imaginary problem as this SO question) attests). Some functions depend on external settings, but not in a surprising way: Sys.time() depends on the system time, but it’s not a surprise: getting the current time is to the whole point of the function! read.csv(path) depends not on the value of path but the contents of the file at that location. Reading from the file system necessarily implies that the results depend on the contents of the file, not its path, so this is not a surprise. Random number generators like runif() peek at the value of the special global variable .Random.seed. This is a little surprising, but if they didn’t have some global state every call to runif() would return the same value. 6.3 Why is it important? Hidden arguments are bad because they make it much harder to predict the output of a fuction. The worst offender by far is the stringsAsFactors option which changes how a number of functions (including data.frame(), as.data.frame(), and read.csv()) treat character vectors. This exists mostly for historical reasons, as described in stringsAsFactors: An unauthorized biography by Roger Peng and stringsAsFactors = &lt;sigh&gt; by Thomas Lumley. ) Allowing the system locale to affect the result of a function is a subtle source of bugs when sharing code between people who work in different countries. To be clear, these defaults on rarely cause problems because most languages that share the same writing system share (most of) the same collation rules. The main exceptions tend to be European languages which have varying rules for modified letters, e.g. in Norwegian, å comes at the end of the alphabet. However, when they do cause problems they will take a long time to track down: you’re unlikely to expect that the coefficients of a linear model are different2 because your code is run in a different country! 6.4 How can I remediate the problem? Generally, hidden arguments are easy to avoid when creating new functions: simply avoid depending on environment variables (like the locale), or global options (like stringsAsFactors). The easiest way for problems to creep in is for you to not realise a function has hidden inputs; make sure to consult the list of common offenders provided above. If you must depend on an environment variable or option, make sure it’s an explicit argument, as in Chapter 16. Such arguments generally should not affect computation (only side-effects like printed output or status messages); if they do affect results, follow Chapter 15 to make sure to inform the user what’s happening. If you have an existing function with a hidden input, you’ll need to take both steps above. First make sure the input is an explicit option, and then make sure it’s printed. For example, lets take as.POSIXct() which basically looks something like this: as.POSIXct &lt;- function(x, tz = &quot;&quot;) { base::as.POSIXct(x, tz = tz) } as.POSIXct(&quot;2020-01-01 09:00&quot;) #&gt; [1] &quot;2020-01-01 09:00:00 UTC&quot; The tz argument is present, but it’s not obvious that \"\" means take from the system timezone. Let’s first make that explicit: as.POSIXct &lt;- function(x, tz = Sys.timezone()) { base::as.POSIXct(x, tz = tz) } as.POSIXct(&quot;2020-01-01 09:00&quot;) #&gt; [1] &quot;2020-01-01 09:00:00 UTC&quot; This is an important argument coming (indirectly) from an environment variable, so we should also print it out if the user hasn’t explicitly set it: as.POSIXct &lt;- function(x, tz = Sys.timezone()) { if (missing(tz)) { message(&quot;Using `tz = &#39;&quot;, tz, &quot;&#39;`&quot;) } base::as.POSIXct(x, tz = tz) } as.POSIXct(&quot;2020-01-01 09:00&quot;) #&gt; Using `tz = &#39;UTC&#39;` #&gt; [1] &quot;2020-01-01 09:00:00 UTC&quot; You’ll get different coefficients for a categorical predictor if the ordering means that a different levels comes first in the alphabet. The predictions and other diagnostics won’t be affected, but you’re likely to be surprised that your coefficients are different.↩︎ "],["args-data-details.html", "7 Data, descriptors, details 7.1 What’s the pattern? 7.2 What are some examples? 7.3 Why is it important? 7.4 How do I avoid the problem? 7.5 How do I remediate past mistakes?", " 7 Data, descriptors, details 7.1 What’s the pattern? Function arguments should always come in the same order: data, then descriptors, then details. Data arguments provide the core data. They are required, and are usually vectors and often determine the type and size of the output. Data arguments are often called data, x, or y. Descriptor arguments describe essential details of the operation, and are usually required. Details arguments control the details of the function. These arguments are optional (because they have default values), and are typically scalars (e.g. na.rm = TRUE, n = 10, prop = 0.1). A standard argument order makes it easier to understand a function at a glance, and this order implies that required arguments always come before optional arguments. Related patterns: ... can play the role of the data argument (i.e. when there are an arbitrary number of inputs), as in paste(). This pattern is best using sparingly, and is described in more detail in Chapter 19. ... can also be used to capture details arguments and pass them on to other functions. See Chapters 18 and 21 to how to use ... as safely as possible in this situation. If the descriptor has a default value, I think you should inform the user about it, as in Chapter 15. 7.2 What are some examples? mean() has one data argument (x) and two details (trim and na.rm). The mathematical (+, -, *, /, …) and comparison (&lt;, &gt;, ==, …) operators have two data arguments. ifelse() has three data arguments (test, yes, no). merge() has two data arguments (x, y), one descriptor (by), and a number of details (all, no.dups, sort, …). rnorm() has no data arguments and three descriptors (n, mean, sd). mean and sd default to 0 and 1 respectively, which makes them feel more like details. I’d argue that they shouldn’t have defaults to make it more clear that they’re descriptors. This would have the side-effect of making rnrorm() more consistent with the other RNGs. In rt(n, df, ncp), however, I think ncp should default to 0 to make it clear that the non-centrality parameter is detail of the t-distribution, not a core part. grepl() has one data argument (x), one descriptor (pattern), and a number of details (fixed, perl, ignore.case, …). stringr::str_detect() has one data argument (string), one descriptor (pattern), and one detail argument (negate). stringr::str_sub() has three data arguments (string, start, and end). You might wonder what makes start and end data arguments, and I admit it took me a while to figure this out too, but I think the crucial factor is that you can give a single string and multiple start/end positions: stringr::str_sub(&quot;Hello&quot;, 1:5, -1) #&gt; [1] &quot;Hello&quot; &quot;ello&quot; &quot;llo&quot; &quot;lo&quot; &quot;o&quot; If I was to write str_sub() today, I’d call the first argument x, and I wouldn’t give start and end default values. ggplot2::ggplot() has one data argument (data) and one descriptor (mapping). lm() has one data argument (data), one descriptor (formula), and many details (weights, na.action, method, …). Unfortunately formula comes before data. This is a historical accident, because putting all model variables into a data frame is a relatively recent innovation in the long life cycle of lm(). purrr::map() has one data argument (.x) and one descriptor (.f). purrr::map2() has two data arguments (.x, .y) and one descrptor (.f). mapply() has any number of data arguments (…), one descriptor (FUN), and a number of details (SIMPLIFY, USE.NAMES, …). The descriptor comes before the data arguments. At first glance it looks like the ggplot2 layer functions, like geom_point(), don’t obey this principle because the first argument is mapping (a descriptor) and the second is data (presumably a data argument). However, this is because ggplot2 doesn’t use the pipe. If it did (like ggplot1), the first argument would be the plot to modify, which is the data object in this case, because the output is also a plot. Here data acts a descriptor, because it modifies the behaviour of the layer. The argument order differs between the layers and ggplot(), because you more commonly specify the data for the plot, and the aesthetic mappings for the layers. This is a little confusing, but I think time has shown it to be a reasonable design decision. 7.3 Why is it important? This convention makes it easy to understand the structure of a function at a glance: the most important arguments are always on the left hand side, and it’s obvious what arguments most affect the shape of the output. Strongly connecting the shape of the first argument to the shape of output is what makes dplyr (data frames), stringr (character vectors), and the map family (vectors) easier to learn. These families of functions represent transformations that preserve the shape while modifying the value. When combined with the pipe, this leads to code that focusses on the transformations, not the objects being transformed. These argument types as also affect how you call a function. As discussed in Chapter 4, you should never name data arguments, and always name details arguments. This convention balances concision with readability. 7.4 How do I avoid the problem? To avoid the problem, you have to carefully analyse the arguments to ensure that you correctly categorise each argument. It’s generally easy to tell the difference between a data argument and a details argument, particularly because data arguments are required and details arguments are optional. But it can be harder to distinguish between data and descriptor, or descriptor and details. This is partly because my categorisation is false trichotomy: there’s really more of a continuous gradient from absolutely required to totally optional than discrete steps. Nevertheless, I think these three categories are useful, and even if you don’t get it absolutely right every time, this framework will help you do better on average. There are a couple of heuristics that you can also check for: Are the arguments generally ordered from most important to least important? If an important argument comes before an unimportant argument, you may have assigned an argument to the wrong category. (Note that this ordering isn’t strict: sometimes it’s more important to organise related arguments together than to precisely order by importance.) Do any arguments with defaults come before any arguments without defaults? This may be a sign that the argument order is wrong, or that you’ve assigned a default value to an required argument (See Chapter @ref(#def-required) for more details.) 7.5 How do I remediate past mistakes? Generally, it is not possible to fix an exported function preserving both old behaviour and new behaviour. Typically, you will need to perform major surgery on the function arguments, and it will convey different conventions about which arguments should be named. This implies that you should deprecate the entire existing function and replace it with a new alternative. Because this is invasive to the user, it’s best to do sparingly: if the mistake is minor, you’re better off waiting until you’ve collected other problems before fixing it. Take tidyr::gather(), for example. It has a number of problems with its design that made them hard to use. Relevant to this chapter, is that the argument order is wrong. "],["args-independence.html", "8 Avoid dependencies between arguments 8.1 What’s the problem? 8.2 What are some examples? 8.3 Why is this important? 8.4 How do I remediate?", " 8 Avoid dependencies between arguments 8.1 What’s the problem? Avoid creating dependencies between details arguments so that only certain combinations are permitted. Dependencies between arguments makes functions harder to use because you have to remember how arguments interact, and you when reading a call, you need to read multiple arguments before interpreting one. 8.2 What are some examples? In rep() you can supply both times and each unless times is a vector: rep(1:3, times = 2, each = 3) #&gt; [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 rep(1:3, times = 1:3, each = 2) #&gt; Error in rep(1:3, times = 1:3, each = 2): invalid &#39;times&#39; argument Learn more in Chapter 11. In var(), na.rm is only used if use is not set. If you supply both use and na.rm, na.rm is silently ignored. In rgamma() you can provide either scale or rate. If you supply both you will get an error or a warning: rgamma(5, shape = 1, rate = 2, scale = 1/2) #&gt; Warning in rgamma(5, shape = 1, rate = 2, scale = 1/2): specify &#39;rate&#39; or #&gt; &#39;scale&#39; but not both #&gt; [1] 0.05501131 0.96119617 0.22807245 0.02848951 0.63576186 rgamma(5, shape = 1, rate = 2, scale = 2) #&gt; Error in rgamma(5, shape = 1, rate = 2, scale = 2): specify &#39;rate&#39; or &#39;scale&#39; but not both grepl() has perl, fixed, and ignore.case arguments which can either be TRUE or FALSE. But fixed = TRUE overrides perl = TRUE, and ignore.case only works if fixed = FALSE. Both fixed and perl change how another argument, pattern, is interpreted. In library() the character.only argument changes how package is intepreted: ggplot2 &lt;- &quot;dplyr&quot; # Loads ggplot2 library(ggplot2) # Loads dplyr library(ggplot2, character.only = TRUE) forcats::fct_lump() decides which algorithm to use based on a combination of the n and prop arguments. In ggplot2::geom_histogram(), you can specify the histogram breaks in three ways: as a number of bins, as the width of each bin (binwidth, plus center or boundary), or the exact breaks. You can only pick one of the three options, which is hard to convey in the documentation. There’s also an implied precedence so that if more than one option is supplied, one will silently win. In readr::locale() there’s a complex dependency between decimal_mark and grouping_mark because they can’t be the same value, and the US and Europe use different standards. See Sections 10.3.1 and 10.3.2 for a two exceptions where the dependency is via specific patterns of missing arguments. 8.3 Why is this important? Having complicated interdependencies between arguments has major downsides: It suggests that there are many more viable code paths than there really are and all those (unnecessary) possibilities still occupy head space. You have to memorise the set of allowed combinations, rather than them being implied by the structure of the function. It increases implementation complexity. Interdependence of arguments suggests complex implementation paths which are harder to analyse and test. It makes documentation harder to write. You have to use extra words to explain exactly how combinations of arguments work together, and it’s not obvious where those words should go. If there’s an interaction between arg_a and arg_b do you document with arg_a, with arg_b, or with both? 8.4 How do I remediate? Often these problems arise because the scope of a function grows over time. When the function was initially designed, the scope was small, and it grew incrementally over time. At no point did it seem worth the additional effort to refactor to a new design, but now you have a large complex function. This makes the problem hard to avoid. To remediate the problem, you’ll need to think holistically and reconsider the complete interface. There are two common outcomes which are illustrated in the case studies below: Splitting the function into multiple functions that each do one thing. Encapulsating related details arguments into a single object. See also larger case study in Chapter 11 where this problem is tangled up with other problems. If these changes to the interface occur to exported functions in a package, you’ll need to consider how to preserve the interface with deprecation warnings. For important functions, it is worth generating an message that includes new code to copy and paste. 8.4.1 Case study: fct_lump() There are many different ways to decide how to lump uncommon factor levels together, and initially we attempted to encode these through arguments to fct_lump(). However, over time as the number of arguments increased, it gets harder and harder to tell what the options are. Currently there are three behaviours: Both n and prop missing - merge together the least frequent levels, ensuring that other is still the smallest level. (For this case, the ties.method argument is ignored.) Only n supplied: if positive, preserves n most common values. Only prop supplied: if positive, preserves Both n and prop supplied: due to a bug in the code, this is treated the same way as both n and prop missing! (But it really should be an error) Would be better to break into three functions: fct_lump_n(f, n) fct_lump_prop(f, prop) fct_lump_smallest(f) That has three advantages: The name of function helps remind you of the purpose. There’s no way to supply both n and prop. The ties.method argument would only appear in fct_lump_n() and _prop(), not _smallest(). 8.4.2 Case study: grepl() vs stringr::str_detect() grepl(), has three arguments that take either FALSE or TRUE: ignore.case, perl, fixed, which might suggest that there are 2 ^ 3 = 16 possible invocations. However, a number of combinations are not allowed: x &lt;- grepl(&quot;a&quot;, letters, fixed = TRUE, ignore.case = TRUE) #&gt; Warning in grepl(&quot;a&quot;, letters, fixed = TRUE, ignore.case = TRUE): argument #&gt; &#39;ignore.case = TRUE&#39; will be ignored x &lt;- grepl(&quot;a&quot;, letters, fixed = TRUE, perl = TRUE) #&gt; Warning in grepl(&quot;a&quot;, letters, fixed = TRUE, perl = TRUE): argument &#39;perl = #&gt; TRUE&#39; will be ignored Part of this problem could be resolved by making it more clear that one important choice is the matching engine to use: POSIX 1003.2 extended regular expressions (the default), Perl-style regular expressions (perl = TRUE) or fixed matching (fixed = TRUE). A better approach would be to use the pattern in Chapter 12, and create a new argument called something like engine = c(\"POSIX\", \"perl\", \"fixed\"). The other problem is that ignore.case can only affect two of the three engines: POSIX and perl. This is hard to remedy without creating a completely new matching engine. Anything to do with case is always harder than you might expect because different languages have different rules. stringr takes a different approach, encoding the engine as an attribute of the pattern: library(stringr) x &lt;- str_detect(letters, &quot;a&quot;) # short for: x &lt;- str_detect(letters, regex(&quot;a&quot;)) x &lt;- str_detect(letters, fixed(&quot;a&quot;)) x &lt;- str_detect(letters, coll(&quot;a&quot;)) This has the advantage that each engine can take different arguments. An alternative approach would be to have a separate engine argument: x &lt;- str_detect(letters, &quot;a&quot;, engine = regex()) x &lt;- str_detect(letters, &quot;a&quot;, engine = fixed()) x &lt;- str_detect(letters, &quot;a&quot;, engine = coll()) This approach is a bit more discoverable (because there’s clearly another argument that affects the pattern), but it’s slightly less general, because of the boundary() engine, which doesn’t match patterns but boundaries: x &lt;- str_detect(letters, boundary(&quot;word&quot;)) # Seems confusing: now you can omit the pattern argument? x &lt;- str_detect(letters, engine = boundary(&quot;word&quot;)) It would also mean that you had an argument engine, that affected how another argument, pattern, was interpreted, so it would repeat the problem in a slightly different form. It’s appealing to all the details of the match wrapped up into a single object. "],["case-study-setnames.html", "9 Case study: setNames() 9.1 What does setNames() do? 9.2 How can we improve the names? 9.3 What about the default values? 9.4 What about bad inputs? 9.5 How could we extend this function? 9.6 Compared to rlang::set_names()", " 9 Case study: setNames() 9.1 What does setNames() do? stats::setNames() is a shorthand that allows you to set vector names inline (it’s a little surprising that it lives in the stats package). It has a simple definition: setNames &lt;- function(object = nm, nm) { names(object) &lt;- nm object } And is easy to use: # Instead of x &lt;- 1:3 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # Can write x &lt;- setNames(1:3, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) x #&gt; a b c #&gt; 1 2 3 This function is short (just two lines of code!) but yields a surprisingly rich analysis. 9.2 How can we improve the names? Firstly, I prefer snake_case to camelCase, so I’d call the function set_names(). Then we need to consider the arguments: I think the first argument, object, would be better called x in order to emphasise that this function only works with vectors (because only vectors have names). The second argument, nm is rather terse, and I don’t see any disadvantage in calling it names. I think you could also argue that it should be called y since its meaning should be obvious from the function name. This yields: set_names &lt;- function(x = names, names) { names(x) &lt;- names x } 9.3 What about the default values? The default values of setNames() are a little hard to understand, because the default value of the first argument is the second argument. It was defined this way to make it possible to name a character vector with itself: setNames(nm = c(&quot;apple&quot;, &quot;banana&quot;, &quot;cake&quot;)) #&gt; apple banana cake #&gt; &quot;apple&quot; &quot;banana&quot; &quot;cake&quot; But that decision leads to a function signature that violates one of the principles of Chapter 7: a required argument comes after an optional argument. Fortunately, we can fix this easily and still preserve the useful ability to name a vector with itself: set_names &lt;- function(x, names = x) { names(x) &lt;- names x } set_names(c(&quot;apple&quot;, &quot;banana&quot;, &quot;cake&quot;)) #&gt; apple banana cake #&gt; &quot;apple&quot; &quot;banana&quot; &quot;cake&quot; This helps to emphasise that x is the primary argument. 9.4 What about bad inputs? Now that we’ve considered how the function works with correct inputs, it’s time to consider how it should work with malformed inputs. The current function checks neither the length not the type: set_names(1:3, &quot;a&quot;) #&gt; a &lt;NA&gt; &lt;NA&gt; #&gt; 1 2 3 set_names(1:3, list(letters[1:3], letters[4], letters[5:6])) #&gt; c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) d c(&quot;e&quot;, &quot;f&quot;) #&gt; 1 2 3 We can resolve this by asserting that the names should always be a character vector, and should have the same length as x: set_names &lt;- function(x, names = x) { if (!is.character(names) || length(names) != length(x)) { stop(&quot;`names` must be a character vector the same length as `x`.&quot;, call. = FALSE) } names(x) &lt;- names x } set_names(1:3, &quot;a&quot;) #&gt; Error: `names` must be a character vector the same length as `x`. set_names(1:3, list(letters[1:3], letters[4], letters[5:6])) #&gt; Error: `names` must be a character vector the same length as `x`. You could also frame this test using vctrs assertions: library(vctrs) set_names &lt;- function(x, names = x) { vec_assert(x) vec_assert(names, ptype = character(), size = length(x)) names(x) &lt;- names x } Note that I slipped in an assertion that x should be a vector. This slightly improves the error message if you accidentally supply the wrong sort of input to set_names(): setNames(mean, 1:3) #&gt; Error in names(object) &lt;- nm: names() applied to a non-vector set_names(mean, 1:3) #&gt; Error in `set_names()`: #&gt; ! `x` must be a vector, not a function. Note that we’re simply checking the length of names here, rather than recycling it, i.e. the invariant is vec_size(set_names(x, y)) is vec_size(x), not vec_size_common(x, y). I think this is the correct behaviour because you usually add names to a vector to create a lookup table, and a lookup table is not useful if there are duplicated names. This makes set_names() less general in return for better error messages when you do something suspicious (and you can always use an explicit rep_along() if do want this behaviour.) 9.5 How could we extend this function? Now that we’ve modified the function so it doesn’t violate the principles in this book, we can think about how we might extend it. Currently the function is only useful for setting names to a constant. Maybe we could extend it to also make it easier to change existing names? One way to do that would be to allow names to be a function: set_names &lt;- function(x, names = x) { vec_assert(x) if (is.function(names)) { names &lt;- names(base::names(x)) } vec_assert(names, ptype = character(), size = length(x)) names(x) &lt;- names x } x &lt;- c(a = 1, b = 2, c = 3) set_names(x, toupper) #&gt; A B C #&gt; 1 2 3 We could also support anonymous function formula shortcut used in many places in the tidyverse. set_names &lt;- function(x, names = x) { vec_assert(x) if (is.function(names) || rlang::is_formula(names)) { fun &lt;- rlang::as_function(names) names &lt;- fun(base::names(x)) } vec_assert(names, ptype = character(), size = length(x)) names(x) &lt;- names x } x &lt;- c(a = 1, b = 2, c = 3) set_names(x, ~ paste0(&quot;x-&quot;, .)) #&gt; x-a x-b x-c #&gt; 1 2 3 Now set_names() supports overriding and modifying names. What about removing them? It turns out that setNames() supported this, but our stricter checks prohibit: x &lt;- c(a = 1, b = 2, c = 3) setNames(x, NULL) #&gt; [1] 1 2 3 set_names(x, NULL) #&gt; Error in `set_names()`: #&gt; ! `names` must be a vector, not NULL. We can fix this with another clause: set_names &lt;- function(x, names = x) { vec_assert(x) if (!is.null(names)) { if (is.function(names) || rlang::is_formula(names)) { fun &lt;- rlang::as_function(names) names &lt;- fun(base::names(x)) } } names(x) &lt;- names x } x &lt;- c(a = 1, b = 2, c = 3) set_names(x, NULL) #&gt; [1] 1 2 3 However, I think this has muddied the logic. To resolve it, I think we should pull out the checking code into a separate function. After trying out a few approaches, I ended up with: check_names &lt;- function(names, x) { if (is.null(names)) { names } else if (vec_is(names)) { vec_assert(names, ptype = character(), size = length(x)) } else if (is.function(names)) { check_names_2(names(base::names(x)), x) } else if (rlang::is.formula(names)) { check_names_2(rlang::as_function(names), x) } else { rlang::abort(&quot;`names` must be NULL, a function or formula, or a vector&quot;) } } This then replaces vec_assert() in set_names(). I separate the input checking and implementation with a blank line to help visually group the parts of the function. set_names &lt;- function(x, names = x) { vec_assert(x) names &lt;- check_names(names, x) names(x) &lt;- names x } We could simplify the function even further, but I think this is a bad idea becaues it mingles input validation with implementation: # Don&#39;t do this set_names &lt;- function(x, names = x) { vec_assert(x) names(x) &lt;- check_names(names, x) x } # Or even set_names &lt;- function(x, names = x) { `names&lt;-`(vec_assert(x), check_names(names, x)) } 9.6 Compared to rlang::set_names() If you’re familiar with rlang, you might notice that we’ve ended up with something rather similar to rlang::set_names(). However, these careful analysis in this chapter has lead to a few differences. rlang::set_names(): Calls the second argument nm, instead of something more descriptive. I think this is simply because we never sat down and fully considered the interface. Coerces nm to character vector. This allows rlang::set_names(1:4) to automatically name the vector, but this seems a relatively weak new feature in return for the cost of not throwing an error message if you provide an unsual vector type. (Both lists and data frames have as.character() methods so this will work for basically any type of vector, even if completely inappropriate.) Passes ... on to function nm. I now think that decision was a mistake: it substantially complicates the interface in return for a relatively small investment. "],["def-required.html", "10 Required args shouldn’t have defaults 10.1 What’s the problem? 10.2 What are some examples? 10.3 What are the exceptions?", " 10 Required args shouldn’t have defaults 10.1 What’s the problem? The absence of a default value should imply than an argument is required; the presence of a default should imply that an argument is optional. When reading a function, it’s important to be able to tell at a glance which arguments must be supplied and which are optional. Otherwise you need to rely on the user having carefully read the documentation. 10.2 What are some examples? In sample() neither x not size has a default value, suggesting that both are required, and the function would error if you didn’t supply them. But size is optional, determined by a complex conditional. sample(1:4) #&gt; [1] 1 2 4 3 sample(4) #&gt; [1] 1 4 3 2 rt() (draw random numbers from the t-distribution) looks like it requires the ncp parameter but it doesn’t. download.file() looks like it requires the method argument but actually consults a global option (download.file.method) if it’s not supplied. lm() does not have defaults for formula, data, subset, weights, na.action, or offset. Only formula is actually required, but even its absence fails to generate a clear error message: lm() #&gt; Error in terms.formula(formula, data = data): argument is not a valid model help() and vignette() have no default for their first argument, suggesting that they’re required. But they’re not: calling help() or vignette() without any arguments lists all help topics and vignettes respectively. In diag(), the argument x has a default 1, but it’s required: if you don’t supply it you get an error: diag() #&gt; Error in diag(): argument &quot;nrow&quot; is missing, with no default diag(x = 1) #&gt; [,1] #&gt; [1,] 1 Conversely, nrow and ncol don’t have defaults but aren’t required. In ggplot2::geom_abline(), slope and intercept don’t have defaults but are not required. If you don’t supply them they default to slope = 1 and intercept = 0, or are taken from aes() if they’re provided there. A common warning sign is the use of missing() inside the function. 10.3 What are the exceptions? There are two exceptions to this rule: A pair of arguments that provide an alternative specification for the same underlying concept. It is only ever possible to supply one argument. When you can either supply one complex object, or a handful of simpler objects. In both cases, I believe the benefits outweigh the costs of violating a standard pattern. 10.3.1 Pair of mututally exclusive arguments A number of functions that allow you to supply exactly one of two possible arguments: read.table() allows you to supply data either with a path to a file, or inline as text. rvest::html_node() allows you to select HTML nodes either with a css selector or an xpath expression. forcats::fct_other() allows you to either keep or drop specified factor values. modelr::seq_range() allows you create a sequence over the range of x by either specifying the length of the sequence (with n) or the distance between values (with by). If you use this technique, use xor() and missing() to check that exactly one argument is supplied: if (!xor(missing(keep), missing(drop))) { stop(&quot;Must supply exactly one of `keep` and `drop`&quot;, call. = FALSE) } And in the documentation, make it clear that only one of the pair can be supplied: #&#39; @param keep,drop Pick one of `keep` and `drop`: #&#39; * `keep` will preserve listed levels, replacing all others with #&#39; `other_level`. #&#39; * `drop` will replace listed levels with `other_level`, keeping all #&#39; as is. This technique should only be used for are exactly two possible arguments. If there are more than two , that is generally a sign you should create more functions. See case studies in Chapter 11 and Section 8.4.1 for examples. 10.3.2 One compound argument vs multiple simple arguments A related, if less generally useful, form is to allow the user to supply either a single complex argument or several smaller arguments. For example: stringr::str_sub(x, cbind(start, end)) is equivalent to str_sub(x, start, end). stringr::str_replace_all(x, c(pattern = replacement)) is equivalent to stringr(x, pattern, replacement). rgb(cbind(r, g, b)) is equivalent to rgb(r, g, b) (See Chapter 17 for more details). options(list(a = 1, b = 2)) is equivalent to options(a = 1, b = 2). The most compelling reason to provide this sort of interface is when another function might return a complex output that you want to use as an input. For example, it seems reasonable that you should be able to feed the output of str_locate() directly into str_sub(): library(stringr) x &lt;- c(&quot;aaaaab&quot;, &quot;aaab&quot;, &quot;ccccb&quot;) loc &lt;- str_locate(x, &quot;a+b&quot;) str_sub(x, loc) #&gt; [1] &quot;aaaaab&quot; &quot;aaab&quot; NA But equally, it would be weird to have to provide a matrix when subsetting with known positions: str_sub(&quot;Hadley&quot;, cbind(2, 4)) #&gt; [1] &quot;adl&quot; So str_sub() allows either individual vectors supplied to start and end, or a two-colummn matrix supplied to start. To implement in your own functions, you should branch on the type of the first argument: (Why? Why not branch if the other arguments are missing? Or some combination?) str_sub &lt;- function(string, start = 1L, end = -1L) { if (is.matrix(start)) { if (!missing(end)) { stop(&quot;`end` must be missing when `start` is a matrix&quot;, call. = FALSE) } if (ncol(start) != 2) { stop(&quot;Matrix `start` must have exactly two columns&quot;, call. = FALSE) } stri_sub(string, from = start[, 1], to = start[, 2]) } else { stri_sub(string, from = start, to = end) } } And make it clear in the documentation: #&#39; @param start,end Integer vectors giving the `start` (default: first) #&#39; and `end` (default: last) positions, inclusively. Alternatively, you #&#39; pass a two-column matrix to `start`, i.e. `str_sub(x, start, end)` #&#39; is equivalent to `str_sub(x, cbind(start, end))` "],["cs-rep.html", "11 Case study: rep() 11.1 What does rep() do? 11.2 What makes this function hard to understand? 11.3 How might we improve the situation? 11.4 Dealing with bad inputs", " 11 Case study: rep() 11.1 What does rep() do? rep() is an extremely useful base R function that repeats a vector x in various ways. It has three details arguments: times, each, and length.out3 that interact in complicated ways. Let’s explore the basics first: x &lt;- c(1, 2, 4) rep(x, times = 3) #&gt; [1] 1 2 4 1 2 4 1 2 4 rep(x, length.out = 10) #&gt; [1] 1 2 4 1 2 4 1 2 4 1 times and length.out replicate the vector in the same way, but length.out allows you to specify a non-integer number of replications. If you specify both, length.out wins. rep(x, times = 3, length.out = 10) #&gt; [1] 1 2 4 1 2 4 1 2 4 1 The each argument repeats individual components of the vector rather than the whole vector: rep(x, each = 3) #&gt; [1] 1 1 1 2 2 2 4 4 4 And you can combine that with times: rep(x, each = 3, times = 2) #&gt; [1] 1 1 1 2 2 2 4 4 4 1 1 1 2 2 2 4 4 4 If you supply a vector to times it works a similar way to each, repeating each component the specified number of times: rep(x, times = x) #&gt; [1] 1 2 2 4 4 4 4 11.2 What makes this function hard to understand? There’s a complicated dependency between times, length.out, and each. times and length.out both control the same underlying variable in different ways, and you can not set them simultaneously. times and each are mostly independent, but if you specify a vector for times you can’t use each. rep(1:3, times = c(2, 2, 2), each = 2) #&gt; Error in rep(1:3, times = c(2, 2, 2), each = 2): invalid &#39;times&#39; argument I think using times with a vector is confusing because it switches from replicating the whole vector to replicating individual values of the vector, like each usually does. rep(1:3, each = 2) #&gt; [1] 1 1 2 2 3 3 rep(1:3, times = 2) #&gt; [1] 1 2 3 1 2 3 rep(1:3, times = c(2, 2, 2)) #&gt; [1] 1 1 2 2 3 3 I think these two problems have the same underlying cause: rep() is trying to do too much in a single function. I think we can make things simpler by turning rep() into two functions: one that replicates the full vector, and one that replicates each element of the vector. 11.3 How might we improve the situation? Two create two new functions, we need to first come up with names: I like rep_each() and rep_full(). rep_each() was a fairly easy name to come up with. rep_full() was a little harder and took a few iterations: I like that full has the same number of letters as each, which makes the two functions look like they belong together. Next, we need to think about their arguments. Both will have a single data argument: x, the vector to replicate. rep_each() has a single details argument which specifies the number of times to replicate each element. rep_time() has two mutually exclusive details arguments, the number of times to repeat the whole vector, or the desired length of the output. What should we call the arguments? We’ve already captured the different replication strategies (each vs. full) in the function name, so I think the argument that specifies the number of times to replicate can be the same, and times seems reasonable. For the second argument to rep_full(), I draw inspiration from rep() which uses length.out. I think it’s obvious that the argument controls the output, so length is adequate. rep_each &lt;- function(x, times) { times &lt;- rep(times, length.out = length(x)) rep(x, times = times) } rep_full &lt;- function(x, times, length) { if (!xor(missing(times), missing(length))) { stop(&quot;Must supply exactly one of `times` and `length`&quot;, call. = FALSE) } if (!missing(times)) { length &lt;- times * base::length(x) } rep(x, length.out = length) } The implementation of rep_full() and rep_each() in terms of rep.int() and rep_len() suggests that R-core members are aware of the problem. (Note the downside of using length as the argument name: we have to call base::length() to avoid evaluating the missing length when times is supplied.) x &lt;- c(1, 2, 4) rep_each(x, times = 2) #&gt; [1] 1 1 2 2 4 4 rep_full(x, times = 2) #&gt; [1] 1 2 4 1 2 4 rep_each(x, times = x) #&gt; [1] 1 2 2 4 4 4 4 rep_full(x, length = 5) #&gt; [1] 1 2 4 1 2 One downside of this approach is if you want to both replicate each component and the entire vector, you have to use two function calls, which is much more verbose than the rep() equivalent. However, I don’t think this is a terribly common use case, and so I think a longer call is more readable. That said, one argument for a single rep function is that rep_each() and rep_full() return the same result if you change their order (i.e. they’re commutative): rep_full(rep_each(x, times = 2), times = 3) #&gt; [1] 1 1 2 2 4 4 1 1 2 2 4 4 1 1 2 2 4 4 rep_each(rep_full(x, times = 3), times = 2) #&gt; [1] 1 1 2 2 4 4 1 1 2 2 4 4 1 1 2 2 4 4 11.4 Dealing with bad inputs The implementations above work well for correct inputs, but will also work without error for a number of incorrect inputs: rep_full(1:3, 1:3) #&gt; Warning in rep(x, length.out = length): first element used of &#39;length.out&#39; #&gt; argument #&gt; [1] 1 2 3 In the code below, I have used vec_assert() and vec_recycle() to make the desired types, sizes, and recycling rules explicit. library(vctrs) rep_each &lt;- function(x, times) { vec_assert(times, numeric()) times &lt;- vec_recycle(times, vec_size(x)) rep.int(x, times) } rep_full &lt;- function(x, times, length) { if (!xor(missing(times), missing(length))) { stop(&quot;Must supply exactly one of `times` and `length`&quot;, call. = FALSE) } else if (!missing(times)) { vec_assert(times, numeric(), 1L) length &lt;- times * base::length(x) } else if (!missing(length)) { vec_assert(length, numeric(), 1L) } rep_len(x, length) } rep_full(1:3, &quot;x&quot;) #&gt; Error in `rep_full()`: #&gt; ! `times` must be a vector with type &lt;double&gt;. #&gt; Instead, it has type &lt;character&gt;. rep_full(1:3, c(1, 2)) #&gt; Error in `rep_full()`: #&gt; ! `times` must have size 1, not size 2. Note that the function specification is rep(x, ...), and times, each, and length.out do not appear explicitly. You have to read the documentation to discover these arguments.↩︎ "],["def-enum.html", "12 Enumerate possible options 12.1 What’s the pattern? 12.2 What are some examples? 12.3 Why is it important? 12.4 How do I use it?", " 12 Enumerate possible options 12.1 What’s the pattern? If the possible values of an argument are a small set of strings, set the default argument to the set of possible values, and then use match.arg() or rlang::arg_match() in the function body. This convention advertises to the user what the possible values, and makes it easy to generate an informative error message for inappropriate inputs. 12.2 What are some examples? In difftime(), units can be any one of “auto”, “secs”, “mins”, “hours”, “days”, or “weeks”. In format(), justify can be “left”, “right”, “center”, or “none”. In trimws(), you can choose which side to remove whitespace from: “both”, “left”, or “right”. In rank(), you can select the ties.method from one of “average”, “first”, “last”, “random”, “max”, or “min”. In RSiteSearch(), you can restrict results to be “functions”, “vignettes”, “views”, or any combination of the three. 12.3 Why is it important? This convention makes it possible to advertise the possible set of values for an argument. The advertisement happens in the function specification, so you see in tooltips and autocomplete, without having to look at the documentation. 12.4 How do I use it? To use this technique, set the default value to a character vector, where the first value is the default. Inside the function, use match.arg() or rlang::arg_match() which checks that the value comes from the known good set. This interface pattern is often coupled with an implementation that uses switch(). Take rank(), for example. The heart of its implementation looks like this: rank &lt;- function(x, ties.method = c(&quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;) ) { ties.method &lt;- match.arg(ties.method) switch(ties.method, average = , min = , max = .Internal(rank(x, length(x), ties.method)), first = sort.list(sort.list(x)), last = sort.list(rev.default(sort.list(x, decreasing = TRUE))), random = sort.list(order(x, stats::runif(length(x)))) ) } x &lt;- c(1, 2, 2, 3, 3, 3) rank(x) #&gt; [1] 1.0 2.5 2.5 5.0 5.0 5.0 rank(x, ties.method = &quot;first&quot;) #&gt; [1] 1 2 3 4 5 6 rank(x, ties.method = &quot;min&quot;) #&gt; [1] 1 2 2 4 4 4 Note that match.arg() will automatically throw an error if the value is not in the set: rank(x, ties.method = &quot;middle&quot;) #&gt; Error in match.arg(ties.method): &#39;arg&#39; should be one of &quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot; It also supports partial matching so that the following code is shorthand for `ties.method = “random”: rank(x, ties.method = &quot;r&quot;) #&gt; [1] 1 3 2 5 4 6 I generally believe that partial matching is a bad idea, because it makes code harder to read. rlang::arg_match() is an alternative to match.args() that doesn’t support partial matching. Instead it provides a helpful error message: rank2 &lt;- function(x, ties.method = c(&quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;) ) { ties.method &lt;- rlang::arg_match(ties.method) rank(x, ties.method = ties.method) } rank2(x, ties.method = &quot;r&quot;) #&gt; Error in `rank2()`: #&gt; ! `ties.method` must be one of &quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, #&gt; &quot;max&quot;, or &quot;min&quot;, not &quot;r&quot;. #&gt; ℹ Did you mean &quot;random&quot;? 12.4.1 How keep defaults short? This technique is a best used when the set of possible values is short. You can see that it’s already getting unwieldy in rank(). If you have a long list of possibilities, there are two options that you could use from Chapter 14. Unfortunately both approaches have major downsides: Set a single default and supply the possible values to match.arg(): rank2 &lt;- function(x, ties.method = &quot;average&quot;) { ties.method &lt;- match.arg( ties.method, c(&quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;) ) } The downside of this approach is that you can no longer see which values are permitted, and you’d have to describe them separately in the documentation. You can, however, still see the default value in the function speci. Store the options in an exported function, and use it in the defaults: ties_method &lt;- function() { c(&quot;average&quot;, &quot;first&quot;, &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;) } rank2 &lt;- function(x, ties.method = ties_method()) { ties.method &lt;- match.arg(ties.method) } The downside of this approach is that when looking at the function spec, you can no longer easily see the default value, or the set of possible values. However, the possible values can easily be found programmatically. This is more worthwhile if you want to share the permitted values across multiple functions. For example stats::p.adjust(), stats::pairwise.prop.test(), stats::pairwise.t.test(), stats::pairwise.wilcox.test() all use p.adjust.method = p.adjust.methods. "],["def-magical.html", "13 Avoid magical defaults 13.1 What’s the problem? 13.2 What are some examples? 13.3 What are the exceptions? 13.4 What causes the problem? 13.5 How do I remediate the problem?", " 13 Avoid magical defaults 13.1 What’s the problem? If a function behaves differently when the default value is supplied explicitly, we say it has a magical default. Magical defaults are best avoided because they make it harder to interpret the function specification. 13.2 What are some examples? In data.frame(), the default argument for row.names is NULL, but if you supply it directly you get a different result: fun_call(data.frame) #&gt; base::data.frame(..., row.names = NULL, check.rows = FALSE, check.names = TRUE, #&gt; fix.empty.names = TRUE, stringsAsFactors = FALSE) x &lt;- setNames(nm = letters[1:3]) data.frame(x) #&gt; x #&gt; a a #&gt; b b #&gt; c c data.frame(x, row.names = NULL) #&gt; x #&gt; 1 a #&gt; 2 b #&gt; 3 c In hist(), the default value of xlim is range(breaks), and the default value for breaks is \"Sturges\". range(\"Sturges\") returns c(\"Sturges\", \"Sturges\") which doesn’t work when supplied explicitly: fun_call(hist.default) #&gt; graphics::hist.default(x, breaks = &quot;Sturges&quot;, freq = NULL, probability = !freq, #&gt; include.lowest = TRUE, right = TRUE, fuzz = 1e-07, density = NULL, #&gt; angle = 45, col = &quot;lightgray&quot;, border = NULL, main = paste(&quot;Histogram of&quot;, #&gt; xname), xlim = range(breaks), ylim = NULL, xlab = xname, #&gt; ylab, axes = TRUE, plot = TRUE, labels = FALSE, nclass = NULL, #&gt; warn.unused = TRUE, ...) hist(1:10, xlim = c(&quot;Sturges&quot;, &quot;Sturges&quot;)) #&gt; Error in plot.window(xlim, ylim, &quot;&quot;, ...): invalid &#39;xlim&#39; value In Vectorize(), the default argument for vectorize.args is arg.names, but this variable is defined inside of Vectorize(), so if you supply it explicitly you get an error. fun_call(Vectorize) #&gt; base::Vectorize(FUN, vectorize.args = arg.names, SIMPLIFY = TRUE, #&gt; USE.NAMES = TRUE) Vectorize(rep.int, vectorize.args = arg.names) #&gt; Error in Vectorize(rep.int, vectorize.args = arg.names): object &#39;arg.names&#39; not found In rbeta(), the default value of ncp is 0, but if you explicitly supply it the function uses a different algorithm: rbeta #&gt; function (n, shape1, shape2, ncp = 0) #&gt; { #&gt; if (missing(ncp)) #&gt; .Call(C_rbeta, n, shape1, shape2) #&gt; else { #&gt; X &lt;- rchisq(n, 2 * shape1, ncp = ncp) #&gt; X/(X + rchisq(n, 2 * shape2)) #&gt; } #&gt; } #&gt; &lt;bytecode: 0x5592d51d0128&gt; #&gt; &lt;environment: namespace:stats&gt; In table(), the default value of dnn is list.names(...); but list.names() is only defined inside of table(). readr::read_csv() has progress = show_progress(), but until version 1.3.1, show_progress() was not exported from the package. That means if you attempted to run it yourself, you’d see an error message: show_progress() #&gt; Error in show_progress(): could not find function &quot;show_progress&quot; In usethis::use_rmarkdown_template(), template_dir has the default value of tolower(asciify(template_name)), but asciify is not exported. That means there’s no way to interactively explore this default value. 13.3 What are the exceptions? It’s ok to use this behaviour when you want the default value of one argument to be the same as another. For example, take rlang::set_names(), which allows you to create a named vector from two inputs: fun_call(set_names) #&gt; rlang::set_names(x, nm = x, ...) set_names(1:3, letters[1:3]) #&gt; a b c #&gt; 1 2 3 The default value for the names is the vector itself. This provides a convenient shortcut for naming a vector with itself: set_names(letters[1:3]) #&gt; a b c #&gt; &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see this same technique in merge(), where all.x and all.y default to the same value as all, and in factor() where labels defaults to the same value as levels. If you use this technique, make sure that you never use the value of an argument that comes later in the argument list. For example, in file.copy() overwrite defaults to the same value as recursive, but the recursive argument is defined after overwrite: fun_call(file.copy) #&gt; base::file.copy(from, to, overwrite = recursive, recursive = FALSE, #&gt; copy.mode = TRUE, copy.date = FALSE) This makes the defaults arguments harder to understand because you can’t just read from left-to-right. 13.4 What causes the problem? There are three primary causes: Overuse of lazy evaluation of default values, which are evaluated in the environment of the function, as described in Advanced R. Here’s a simple example: f1 &lt;- function(x = y) { y &lt;- trunc(Sys.time(), units = &quot;months&quot;) x } y &lt;- 1 f1() #&gt; [1] &quot;2022-07-01 UTC&quot; f1(y) #&gt; [1] 1 When x takes the value y from its default, it’s evaluated inside the function, yielding 1. When y is supplied explicitly, it is evaluated in the caller environment, yielding 2. Use of missing() so that the default value is never consulted: f2 &lt;- function(x = 1) { if (missing(x)) { 2 } else { x } } f2() #&gt; [1] 2 f2(1) #&gt; [1] 1 In packages, it’s easy to use a non-exported function without thinking about it. This function is available to you, the package author, but not the user of the package, which makes it harder for them to understand how a package works. 13.5 How do I remediate the problem? This problem is generally easy to avoid for new functions: Don’t use default values that depend on variables defined inside the function. Don’t use missing()4. Don’t use unexported functions. If you have a made a mistake in an older function you can remediate it by using a NULL default, as described in Chapter 14. If the problem is caused by an unexported function, you can also choose to document and export it. `%||%` &lt;- function(x, y) if (is.null(x)) y else x f1_better &lt;- function(x = NULL) { y &lt;- trunc(Sys.time(), units = &quot;weeks&quot;) x &lt;- x %||% y x } f2_better &lt;- function(x = NULL) { x &lt;- x %||% 2 x } This modification should not break existing code, because expands the function interface: all previous code will continue to work, and the function will also work if the argument is passed NULL input (which probably didn’t previously). For functions like data.frame() where NULL is already a permissible value, you’ll need to use a sentinel object, as described in Section 14.4.3. sentinel &lt;- function() structure(list(), class = &quot;sentinel&quot;) is_sentinel &lt;- function(x) inherits(x, &quot;sentinel&quot;) data.frame_better &lt;- function(..., row.names = sentinel()) { if (is_sentinel(row.names)) { # old default behaviour } } The only exceptions are described in Sections 10.3.1 and 10.3.2.↩︎ "],["def-short.html", "14 Keep defaults short and sweet 14.1 What’s the pattern? 14.2 What are some examples? 14.3 Why is it important? 14.4 How do I use it? 14.5 How do I remediate existing problems?", " 14 Keep defaults short and sweet 14.1 What’s the pattern? Default values should be short and sweet. This makes the function specification easier to scan. 14.2 What are some examples? The following examples, drawn from base R, illustrate some functions that don’t follow this pattern: sample.int() uses a complicated rule to determine whether or not to use a faster hash based method that’s only applicable in some circumstances: useHash = (!replace &amp;&amp; is.null(prob) &amp;&amp; size &lt;= n/2 &amp;&amp; n &gt; 1e+07)) exists(), which figures out if a variable exists in a given environment, uses a complex default to determine which environment to look in if not specifically provided: envir = (if (missing(frame)) as.environment(where) else sys.frame(frame)) (NB: ?exists cheats and hides the long default in the documentation.) reshape() has the longest default argument in the base and stats packages. The split argument is one of two possible lists depending on the value of the sep argument: reshape( split = if (sep == &quot;&quot;) { list(regexp = &quot;[A-Za-z][0-9]&quot;, include = TRUE) } else { list(regexp = sep, include = FALSE, fixed = TRUE) }) ) 14.3 Why is it important? 14.4 How do I use it? There are three approaches: Set the default value to NULL and calculate the default only when the argument is NULL. Providing a default of NULL signals that the argument is optional (Chapter 10) but that the default requires some calculation. If the calculation is complex, and the user might find it useful in other scenarios, compute it with an exported function that documents exactly what happens. If NULL is meaningful, so you can’t use the first approach, use a “sentinel” object instead. 14.4.1 NULL default The most common approach is to use NULL as a sentinel value that indicates that the argument is optional, but non-trivial. This pattern is made substantially more elegant with the infix %||% operator. You can either get it by importing it from rlang, or copying and pasting it in to your utils.R: `%||%` &lt;- function(x, y) if (is.null(x)) y else x This allows you to write code like this (extracted from ggplot2::geom_bar()). It computes the width by first looking at the data, then in the paramters, finally falling back to computing it from the resolution of the x variable: width &lt;- data$width %||% params$width %||% (resolution(data$x, FALSE) * 0.9) Or this code from the colourbar legend: it finds the horizontal justification by first looking in the guide settings, then in the specific theme setting, then then title element, finally using 0 if nothing else is set: title.hjust &lt;- guide$title.hjust %||% theme$legend.title.align %||% title.theme$hjust %||% 0 As you can see, %||% is particularly well suited to arguments where the default value is found through a cascading system of fallbacks. Don’t use %||% for more complex examples. For example in reshape() I would set split = NULL and then write: if (is.null(split)) { if (sep == &quot;&quot;) { split &lt;- list(regexp = &quot;[A-Za-z][0-9]&quot;, include = TRUE) } else { split &lt;- list(regexp = sep, include = FALSE, fixed = TRUE) } } (I would probably also switch on is.null(sep) too, to make it more clear that there is special behaviour.) 14.4.2 Helper function For more complicated cases, you’ll probably want to pull the code that computes the default out into a separate function, and in many cases you’ll want to export (and document) the function. A good example of this pattern is readr::show_progress(): it’s used in every read_ function in readr and it’s sufficiently complicated that you don’t want to copy and paste it between functions. It’s also nice to document it in its own file, rather than cluttering up file reading functions with incidental details. 14.4.3 Sentinel value Sometimes a default argument has a complex calculation that you don’t want to include in arguments list. You’d normally use NULL to indicate that it’s calculated by default, but NULL is a meaningful option. In that case, you can use a sentinel object. str(ggplot2::waiver()) #&gt; list() #&gt; - attr(*, &quot;class&quot;)= chr &quot;waiver&quot; str(purrr::done()) #&gt; List of 1 #&gt; $ : symbol #&gt; - attr(*, &quot;class&quot;)= chr [1:2] &quot;rlang_box_done&quot; &quot;rlang_box&quot; #&gt; - attr(*, &quot;empty&quot;)= logi TRUE str(rlang::zap()) #&gt; list() #&gt; - attr(*, &quot;class&quot;)= chr &quot;rlang_zap&quot; Or is this the wrong way around? Should the default always be NULL and we have a special value to use when you actually want a NULL? Take purrr::reduce(): it has an optional details argument called init. When supplied, it serves as the initial value for the computation. But any value (including NULL) can a valid value. And using a sentinel value for this one case seemed like overkill. 14.5 How do I remediate existing problems? If you have a function with a long default, you can use any of the three approaches above to remediate it. As long as you don’t accidentally change the default value, this does not affect the function interface. Make sure you have a test for the default operation of the function before embarking on this change. # BEFORE sample.int &lt;- function(n, size = n, replace = FALSE, prob = NULL, useHash = (!replace &amp;&amp; is.null(prob) &amp;&amp; size &lt;= n/2 &amp;&amp; n &gt; 1e+07) ) { if (useHash) { .Internal(sample2(n, size)) } else { .Internal(sample(n, size, replace, prob)) } } # AFTER sample.int &lt;- function(n, size = n, replace = FALSE, prob = NULL, useHash = NULL) { useHash &lt;- useHash %||% !replace &amp;&amp; is.null(prob) &amp;&amp; size &lt;= n/2 &amp;&amp; n &gt; 1e+07 if (useHash) { .Internal(sample2(n, size)) } else { .Internal(sample(n, size, replace, prob)) } } "],["def-inform.html", "15 Explain important defaults 15.1 What’s the pattern? 15.2 What are some examples? 15.3 Why is it important? 15.4 How can I use it?", " 15 Explain important defaults 15.1 What’s the pattern? If a default value is important, and the computation is non-trivial, inform the user what value was used. This is particularly important when the default value is an educated guess, and you want the user to change it. It is also important when descriptor arguments (Chapter 7) have defaults. 15.2 What are some examples? dplyr::left_join() and friends automatically compute the variables to join by as the variables that occur in both x and y (this is called a natural join in SQL). This is convenient, but it’s a heuristic so doesn’t always work. library(nycflights13) library(dplyr) # Correct out &lt;- left_join(flights, airlines) #&gt; Joining, by = &quot;carrier&quot; # Incorrect out &lt;- left_join(flights, planes) #&gt; Joining, by = c(&quot;year&quot;, &quot;tailnum&quot;) # Error out &lt;- left_join(flights, airports) #&gt; Error in `left_join()`: #&gt; ! `by` must be supplied when `x` and `y` have no common variables. #&gt; ℹ use by = character()` to perform a cross-join. readr::read_csv() reads a csv file into a data frame. Because csv files don’t store the type of each variable, readr must guess the types. In order to be fast, read_csv() uses some heuristics, so it might guess wrong. Or maybe guesses correctly today, but when your automated script runs in two months time when the data format has changed, it might guess incorrectly and give weird downstream errors. For this reason, read_csv() prints the column specification in a way that you can copy-and-paste into your code. library(readr) mtcars &lt;- read_csv(readr_example(&quot;mtcars.csv&quot;)) #&gt; Rows: 32 Columns: 11 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; dbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In ggplot2::geom_histogram(), the binwidth is an important parameter that you should always experiment with. This suggests it should be a required argument, but it’s hard to know what values to try until you’ve seen a plot. For this reason, ggplot2 provides a suboptimal default of 30 bins: this gets you started, and then a message tells you how to modify. library(ggplot2) ggplot(diamonds, aes(carat)) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. When installing packages, install.packages() informs of the value of the lib argument, which defaults to .libPath()[[1]]: install.packages(&quot;forcats&quot;) # Installing package into ‘/Users/hadley/R’ # (as ‘lib’ is unspecified) This, however, is not terribly important (most people only use one library), it’s easy to ignore this amongst the other output, and the message doesn’t refer to the mechanism that controls the default (.libPaths()). 15.3 Why is it important? There are two ways to fire a machine gun in the dark. You can find out exactly where your target is (range, elevation, and azimuth). You can determine the environmental conditions (temperature, humidity, air pressure, wind, and so on). You can determine the precise specifications of the cartridges and bullets you are using, and their interactions with the actual gun you are firing. You can then use tables or a firing computer to calculate the exact bearing and elevation of the barrel. If everything works exactly as specified, your tables are correct, and the environment doesn’t change, your bullets should land close to their target. Or you could use tracer bullets. Tracer bullets are loaded at intervals on the ammo belt alongside regular ammunition. When they’re fired, their phosphorus ignites and leaves a pyrotechnic trail from the gun to whatever they hit. If the tracers are hitting the target, then so are the regular bullets. — The Pragmatic Programmer I think this is a valuable pattern because it helps balance two tensions in function design: Forcing the function user to really think about what they want to do. Trying to be helpful, so the user of function can achieve their goal as quickly as possible. Often your thoughts about a problem will be aided by a first attempt, even if that attempt is wrong. Helps facilitate iteration: you don’t sit down and contemplate for an hour and then write one perfectly formed line of R code. You take a stab at it, look at the result, and then tweak. Taking a default that the user really should carefully think about and make a decision on, and turning it into a heurstic or educated guess, and reporting the value, is like a tracer bullet. The counterpoint to this pattern is that people don’t read repeated output. For example, do you know how to cite R in a paper? It’s mentioned every time that you start R. Human brains are extremely good at filtering out unchanging signals, which means that you must use this technique with caution. If every argument tells you the default it uses, it’s effectively the same as doing nothing: the most important signals will get buried in the noise. This is why you’ll see the technique used in only a handful of places in the tidyverse. 15.4 How can I use it? To use this message you need to generate a message from the computation of the default value. The easiest way to do this to write a small helper function. It should compute the default value given some inputs and generate a message() that gives the code that you could copy and paste into the function call. Take the dplyr join functions, for example. They use a function like this: common_by &lt;- function(x, y) { common &lt;- intersect(names(x), names(y)) if (length(common) == 0) { stop(&quot;Must specify `by` when no common variables in `x` and `y`&quot;, call. = FALSE) } message(&quot;Computing common variables: `by = &quot;, rlang::expr_text(common), &quot;`&quot;) common } common_by(data.frame(x = 1), data.frame(x = 1)) #&gt; Computing common variables: `by = &quot;x&quot;` #&gt; [1] &quot;x&quot; common_by(flights, planes) #&gt; Computing common variables: `by = c(&quot;year&quot;, &quot;tailnum&quot;)` #&gt; [1] &quot;year&quot; &quot;tailnum&quot; The technique you use to generate the code will vary from function to function. rlang::expr_text() is useful here because it automatically creates the code you’d use to build the character vector. To avoid creating a magical default (Chapter 13), either export and document the function, or use the technique of Section 14.4.1: left_join &lt;- function(x, y, by = NULL) { by &lt;- by %||% common_by(x, y) } "],["def-user.html", "16 User settable defaults 16.1 What’s the pattern? 16.2 What are some examples? 16.3 Why is it important? 16.4 What are the exceptions? 16.5 How do I use it?", " 16 User settable defaults 16.1 What’s the pattern? It’s sometimes useful to give the user control over default values, so that they can set once per session or once for every session in their .Rprofile. To do so, use getOption() in the default value. Note that this pattern should general only be used to control the side-effects of a function, not its compute value. The two primary uses are for controlling the apperance of output, particulary in print() methods, and for setting default values in generated templates. Related patterns: If a global option affects the results of the computation (not just its side-effects), you have an example of Chapter 6. 16.2 What are some examples? 16.3 Why is it important? 16.4 What are the exceptions? 16.5 How do I use it? "],["cs-rgb.html", "17 Case study: rgb()", " 17 Case study: rgb() Interface: Function name and argument names. alpha has no default but isn’t required. names not required (imo). maxColorValue doens’t have most useful default, and not really needed (imo). Data frame rather than matrix. Error if function specification is correct Check for data type, not missingness. library(rlang) rgba &lt;- function(r, g, b, a = NULL) { if (is.data.frame(r)) { df &lt;- r if (!ncol(df) %in% c(3L, 4L)) { abort(&quot;If `r` is data frame, it must have 3 or 4 columns.&quot;) } if (!missing(b) || !missing(g) || !missing(a)) { abort(&quot;If `r` is a data frame, `b`, `g`, and `a` must not be set.&quot;) } r &lt;- df[[1L]] g &lt;- df[[2L]] b &lt;- df[[3L]] if (ncol(df) == 4) { a &lt;- df[[4L]] } } rgb(r, g, b, alpha = a, maxColorValue = 255) } rgba(16, 16, 16) #&gt; [1] &quot;#101010&quot; rgba(data.frame(16, 16, 16)) #&gt; [1] &quot;#101010&quot; rgba(data.frame(16, 16)) #&gt; Error in `rgba()`: #&gt; ! If `r` is data frame, it must have 3 or 4 columns. rgba(data.frame(16, 16, 16), 1) #&gt; Error in `rgba()`: #&gt; ! If `r` is a data frame, `b`, `g`, and `a` must not be set. rgba &lt;- function(r, g, b, a = NULL) { if (is.data.frame(r)) { df &lt;- r if (!all(c(&quot;r&quot;, &quot;g&quot;, &quot;b&quot;)) %in% names(df)) { abort(&quot;If first argument is a data frame, it must have r, g, and b columns.&quot;) } if (!missing(b) || !missing(g) || !missing(a)) { abort(&quot;If `r` is a data frame, `b`, `g`, and `a` must not be set.&quot;) } } else { # Handles vectorisation df &lt;- tibble(r = r, g = g, b = b, a = a) } # Assumes this function checks types and gives informative error messages rgb(df$r, df$g, df$b, alpha = df$a, maxColorValue = 255) } "],["dots-position.html", "18 Data, dots, details 18.1 What’s the pattern? 18.2 What are some examples? 18.3 What is it important? 18.4 How do I do it? 18.5 How do I remediate it?", " 18 Data, dots, details 18.1 What’s the pattern? When you use ... in a function, where should you put it? It’s obvious that the data arguments must come first. But which should come next, the dots or the details? This pattern tells you to place ... between the data arguments (the required arguments that supply the key “data” to the function) and the details arguments (optional additional arguments that control the finer details of the function). 18.2 What are some examples? Many functions in base R take data, then details, then dots: args(unique) #&gt; function (x, incomparables = FALSE, ...) #&gt; NULL args(median) #&gt; function (x, na.rm = FALSE, ...) #&gt; NULL This doesn’t cause many problems because most people will fully spell out the names of details arguments. However, there are other summary functions that take the data via dots, then details. sum(2, 3, 10) #&gt; [1] 15 prod(2, 3, 10) #&gt; [1] 60 args(sum) #&gt; function (..., na.rm = FALSE) #&gt; NULL args(prod) #&gt; function (..., na.rm = FALSE) #&gt; NULL This allows these functions to take any number of input vectors, but these two different interfaces make it very easy for users to construct calls that are technically valid, but that don’t return the desired result. If you’re expecting median() to work like sum(), you might call it the same way: median(2, 3, 10) #&gt; [1] 2 This silently returns an incorrect result because median() has arguments x, na.rm, and .... The user must remember that median() – and mean() too! – require data that is packed into a single vector. median(c(2, 3, 10)) #&gt; [1] 3 mean(c(2, 3, 10)) #&gt; [1] 5 18.3 What is it important? There are three primary advantages: It forces the user of your function to fully name the detail arguments, because arguments that come after ... are never matched by position or partially by name. Using full names for details arguments is good practice, because it makes code easier to read. You can easily add new detail arguments without changing the meaning of existing function calls. This makes it easy to extend your function with new capabilities, because you don’t need to worry about changing existing code. When coupled with “inspect the dots” (Chapter 21), or “dot prefix” (Chapter 20) it minimises the chances that misspelled arguments names will silently go astray. The payoff of this pattern is not huge: it protects against a fairly unusual failure mode. However, the failure mode is silent (so easy to miss), and the pattern is very easy to apply, so I think the payoff is still worth it. 18.4 How do I do it? Following this pattern is simple: just identity which arguments are data, and which arguments are details and then put the … in between. 18.5 How do I remediate it? If you’ve already published a function where you’ve put ... in the wrong place, it’s easy to fix. You’ll need to use function from the ellipsis package to check that ... is as expected (e.g. from Chapters 21 or 19). Since using the full names for details arguments is good practice, making this change will typically affect little existing code, but it is an interface change so should be advertised prominently. old_interface &lt;- function(x, data1 = 1, data2 = 2, ...) { } new_interace &lt;- function(x, ..., data1 = 1, data2 = 2) { ellipsis::check_dots_used() } We can use this approach to make a safer version of median(): safe_median &lt;- function(x, ..., na.rm = FALSE) { ellipsis::check_dots_used() median(x, ..., na.rm = na.rm) } safe_median(2, 3, 10) #&gt; Error: #&gt; ! Arguments in `...` must be used. #&gt; ✖ Problematic arguments: #&gt; • ..1 = 3 #&gt; • ..2 = 10 "],["dots-data.html", "19 Making data with … 19.1 What’s the problem? 19.2 What are some examples? 19.3 Why is it important? 19.4 What are the exceptions? 19.5 How can remediate it? 19.6 How can I protect myself? 19.7 Selecting variables", " 19 Making data with … 19.1 What’s the problem? A number of functions take ... to save the user from having to create a vector themselves: 19.2 What are some examples? sum(c(1, 1, 1)) #&gt; [1] 3 # can be shortened to: sum(1, 1, 1) #&gt; [1] 3 f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), levels = c(&quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;a&quot;)) f #&gt; [1] a b c d #&gt; Levels: b c d a fct_relevel(f, c(&quot;b&quot;, &quot;a&quot;)) #&gt; [1] a b c d #&gt; Levels: b a c d # can be shortened to: fct_relevel(f, &quot;b&quot;, &quot;a&quot;) #&gt; [1] a b c d #&gt; Levels: b a c d mapply() 19.3 Why is it important? In general, I think it is best to avoid using ... for this purpose because it has a relatively small benefit, only reducing typing by three letters c(), but has a number of costs: It can give the misleading impression that other functions in the same family work the same way. For example, if you’re internalised how sum() works, you might predict that mean() works the same way, but it does not: mean(c(1, 2, 3)) #&gt; [1] 2 mean(1, 2, 3) #&gt; [1] 1 (See Chapter 18 to learn why this doesn’t give an error message.) It makes it harder to adapt the function for new uses. For example, fct_relevel() can also be called with a function: fct_relevel(f, sort) #&gt; [1] a b c d #&gt; Levels: a b c d If fct_relevel() took its input as a single vector, you could easily extend it to also work with functions: fct_relevel &lt;- function(f, x) { if (is.function(x)) { x &lt;- f(levels(x)) } } However, because fct_relevel() uses dots, the implementation needs to be more complicated: fct_relevel &lt;- function(f, ...) { if (dots_n(...) == 1L &amp;&amp; is.function(..1)) { levels &lt;- fun(levels(x)) } else { levels &lt;- c(...) } } 19.4 What are the exceptions? Note that in all the examples above, the ... are used to collect a single details argument. It’s ok to use ... to collect data, as in paste(), data.frame(), or list(). 19.5 How can remediate it? If you’ve already published a function where you’ve used ... for this purpose you can change the interface by adding a new argument in front of ..., and then warning if anything ends up in .... old_foo &lt;- function(x, ...) { } new_foo &lt;- function(x, y, ...) { if (rlang::dots_n(...) &gt; 0) { warning(&quot;Use of `...` is now deprecated. Please put all arguments in `y`&quot;) y &lt;- c(y, ...) } } Because this is a interface change, it should be prominently advertised in packages. 19.6 How can I protect myself? If you do feel that the tradeoff is worth it (i.e. it’s an extremely frequently used function and the savings over time will be considerable), you need to take some steps to minimise the downsides. This is easiest if you’re constructing a vector that shouldn’t have names. In this case, you can call ellipsis::check_dots_unnamed() to ensure that no named arguments have been accidentally passed to .... This protects you against the following undesirable behaviour of sum(): sum(1, 1, 1, na.omit = TRUE) #&gt; [1] 4 safe_sum &lt;- function(..., na.rm = TRUE) { ellipsis::check_dots_unnamed() sum(c(...), na.rm = na.rm) } safe_sum(1, 1, 1, na.omit = TRUE) #&gt; Error in `safe_sum()`: #&gt; ! Arguments in `...` must be passed by position, not name. #&gt; ✖ Problematic argument: #&gt; • na.omit = TRUE If you want your vector to have names, the problem is harder, and there’s relatively little that you can. You’ll need to ensure that all other arguments get a . prefix (to minimise chances of a mismatch) and then think carefully about how you might detect problems by thinking about the expect type of c(...). As far as I know, there are no general techniques, and you’ll have to think about the problem on a case-by-case basis. 19.7 Selecting variables A number of funtions in the tidyverse use ... for selecting variables. For example, tidyr::fill() lets you fill in missing values based on the previous row: df &lt;- tribble( ~year, ~month, ~day, 2020, 1, 1, NA, NA, 2, NA, NA, 3, NA, 2, 1 ) df %&gt;% fill(year, month) #&gt; # A tibble: 4 × 3 #&gt; year month day #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2020 1 1 #&gt; 2 2020 1 2 #&gt; 3 2020 1 3 #&gt; 4 2020 2 1 All functions that work like this include a call to tidyselect::vars_select() that looks something like this: find_vars &lt;- function(data, ...) { tidyselect::vars_select(names(data), ...) } find_vars(df, year, month) #&gt; year month #&gt; &quot;year&quot; &quot;month&quot; I now think that this interface is a mistake because it suffers from the same problem as sum(): we’re using ... to only save a little typing. We can eliminate the use of dots by requiring the user to use c(). (This change also requires explicit quoting and unquoting of vars since we’re no longer using ....) foo &lt;- function(data, vars) { tidyselect::vars_select(names(data), !!enquo(vars)) } find_vars(df, c(year, month)) #&gt; year month #&gt; &quot;year&quot; &quot;month&quot; In other words, I believe that better interface to fill() would be: df %&gt;% fill(c(year, month)) Other tidyverse functions like dplyr’s scoped verbs and ggplot2::facet_grid() require the user to explicitly quote the input. I now believe that this is also a suboptimal interface because it is more typing (var() is longer than c(), and you must quote even single variables), and arguments that require their inputs to be explicitly quoted are rare in the tidyverse. # existing interface dplyr::mutate_at(mtcars, vars(cyl:vs), mean) # what I would create today dplyr::mutate_at(mtcars, c(cyl:vs), mean) # existing interface ggplot2::facet_grid(rows = vars(drv), cols = vars(vs, am)) # what I would create today ggplot2::facet_grid(rows = drv, cols = c(vs, am)) That said, it is unlikely we will ever change functions, because the benefit is smaller (primarily improved consistency) and the costs are high, as it impossible to switch from an evaluated argument to a quoted argument without breaking backward compatibility in some small percentage of cases. "],["dots-prefix.html", "20 Dot prefix 20.1 What’s the pattern? 20.2 What are some examples? 20.3 Case study: dplyr verbs 20.4 Other approaches in base R 20.5 What are the exceptions?", " 20 Dot prefix 20.1 What’s the pattern? When using ... to create a data structure, or when passing ... to a user-supplied function, add a . prefix to all named arguments. This reduces (but does not eliminate) the chances of matching an argument at the wrong level. Additionally, you should always provide some mechanism that allows you to escape and use that name if needed. library(purrr) (Not important if you ignore names: e.g. cat().) 20.2 What are some examples? Look at the arguments to some functions in purrr: args(map) #&gt; function (.x, .f, ...) #&gt; NULL args(reduce) #&gt; function (.x, .f, ..., .init, .dir = c(&quot;forward&quot;, &quot;backward&quot;)) #&gt; NULL args(detect) #&gt; function (.x, .f, ..., .dir = c(&quot;forward&quot;, &quot;backward&quot;), .right = NULL, #&gt; .default = NULL) #&gt; NULL Notice that all named arguments start with .. This reduces the chance that you will incorrectly match an argument to map(), rather than to an argument of .f. Obviously it can’t eliminate it. Escape mechanism is the anonymous function. Little easier to access in purrr::map() since you can create with ~, which is much less typing than function() {}. For example, imagine you want to… Example: https://jennybc.github.io/purrr-tutorial/ls02_map-extraction-advanced.html#list_inside_a_data_frame 20.3 Case study: dplyr verbs args(dplyr::filter) #&gt; function (.data, ..., .preserve = FALSE) #&gt; NULL args(dplyr::group_by) #&gt; function (.data, ..., .add = FALSE, .drop = group_by_drop_default(.data)) #&gt; NULL Escape hatch is :=. Ooops: args(dplyr::left_join) #&gt; function (x, y, by = NULL, copy = FALSE, suffix = c(&quot;.x&quot;, &quot;.y&quot;), #&gt; ..., keep = FALSE) #&gt; NULL 20.4 Other approaches in base R Base R uses two alternative methods: uppercase and _ prefix. The apply family tends to use uppercase function names for the same reason. Unfortunately the functions are a little inconsitent which makes it hard to see this pattern. I think a dot prefix is better because it’s easier to type (you don’t have to hold down the shift-key with one finger). args(lapply) #&gt; function (X, FUN, ...) #&gt; NULL args(sapply) #&gt; function (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE) #&gt; NULL args(apply) #&gt; function (X, MARGIN, FUN, ..., simplify = TRUE) #&gt; NULL args(mapply) #&gt; function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) #&gt; NULL args(tapply) #&gt; function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE) #&gt; NULL Reduce() and friends avoid the problem altogether by not accepting ..., and requiring that the user creates anonymous functions. But this is verbose, particularly without shortcuts to create functions. transform() goes a step further and uses an non-syntactic variable name. args(transform) #&gt; function (`_data`, ...) #&gt; NULL Using a non-syntactic variable names means that it must always be surrounded in `. This means that a user is even less likely to use it that with ., but it increases friction when writing the function. In my opinion, this trade-off is not worth it. 20.5 What are the exceptions? tryCatch(): the names give classes so, as long as you don’t create a condition class called expr or finally (which would be weird!) you don’t need to worry about matches "],["dots-inspect.html", "21 Inspect the dots 21.1 What’s the pattern? 21.2 What are some examples? 21.3 How do I do it?", " 21 Inspect the dots 21.1 What’s the pattern? Whenever you use ... in an S3 generic to allow methods to add custom arguments, you should inspect the dots to make sure that every argument is used. You can also use this same approach when passing ... to an overly permissive function. 21.2 What are some examples? If you don’t use this technique it is easy to end up with functions that silently return the incorrect result when argument names are misspelled. # Misspelled weighted.mean(c(1, 0, -1), wt = c(10, 0, 0)) #&gt; [1] 0 mean(c(1:9, 100), trim = 0.1) #&gt; [1] 5.5 # Correct weighted.mean(c(1, 0, -1), w = c(10, 0, 0)) #&gt; [1] 1 mean(c(1:9, 100), trim = 0.1) #&gt; [1] 5.5 21.3 How do I do it? Add a call to ellipsis::check_dots_used() in the generic before the call to UseMethod(). This automatically adds an on exit handler, which checks that ever element of ... has been evaluated just prior to the function returnning. You can see this in action by creating a safe wrapper around cut(), which has different arguments for its numeric and date methods. safe_cut &lt;- function(x, breaks, ..., right = TRUE) { ellipsis::check_dots_used() UseMethod(&quot;safe_cut&quot;) } safe_cut.numeric &lt;- function(x, breaks, ..., right = TRUE, include.lowest = FALSE) { cut(x, breaks = breaks, right = right, include.lowest = include.lowest) } safe_cut.Date &lt;- function(x, breaks, ..., right = TRUE, start.on.monday = TRUE) { cut(x, breaks = breaks, right = right, start.on.monday = start.on.monday) } 21.3.1 What are the limitations? Accurately detecting this problem is hard because no one place has all the information needed to tell if an argument is superfluous or not (the precise details are beyond the scope of this text). Instead the ellipsis package takes advantage of R’s lazy evaluation and inspects the internal components of ... to see if their evaluation has been forced. If a function is called primarily for its side-effects, the error will occur after the side-effect has happened, making for a confusing result. Here the best we can do is a warning, generated by ellipsis::warn_dots_unused() If a function captures the components of ... using enquo() or match.call(), you can not use this technique. This also means that if you use check_dots_used(), the method author can not choose to add a quoted argument. I think this is ok because quoting vs. evaluating is part of the interface of the generic, so methods should not change this interface, and it’s fine for the author of the generic to make that decision for all method authors. 21.3.2 What are other uses? This same technique can also be used when you are wrapping other functions. For example, stringr::str_sort() takes ... and passes it on to stringi::stri_opts_collator(). As of March 2019, str_sort() looked like this: str_sort &lt;- function(x, decreasing = FALSE, na_last = TRUE, locale = &quot;en&quot;, numeric = FALSE, ...) { stringi::stri_sort(x, decreasing = decreasing, na_last = na_last, opts_collator = stringi::stri_opts_collator( locale, numeric = numeric, ... ) ) } x &lt;- c(&quot;x1&quot;, &quot;x100&quot;, &quot;x2&quot;) str_sort(x) #&gt; [1] &quot;x1&quot; &quot;x100&quot; &quot;x2&quot; str_sort(x, numeric = TRUE) #&gt; [1] &quot;x1&quot; &quot;x2&quot; &quot;x100&quot; This is wrapper is useful because it decouples str_sort() from the stri_opts_collator() meaning that if stri_opts_collator() gains new arguments users of str_sort() can take advantage of them immediately. But most of the arguments in stri_opts_collator() are sufficiently arcane that they don’t need to be exposed directly in stringr, which is designed to minimise the cognitive load of the user, by hiding some of the full complexity of string handling. (The importance of the locale argument comes up in “hidden inputs”, Chapter 6.) However, stri_opts_collator() deliberately ignores any arguments in .... This means that misspellings are silently ignored: str_sort(x, numric = TRUE) #&gt; Warning in stringi::stri_opts_collator(locale, numeric = numeric, ...): Unknown #&gt; option to `stri_opts_collator`. #&gt; [1] &quot;x1&quot; &quot;x100&quot; &quot;x2&quot; We can work around this behaviour by adding check_dots_used() to str_sort(): str_sort &lt;- function(x, decreasing = FALSE, na_last = TRUE, locale = &quot;en&quot;, numeric = FALSE, ...) { ellipsis::check_dots_used() stringi::stri_sort(x, decreasing = decreasing, na_last = na_last, opts_collator = stringi::stri_opts_collator( locale, numeric = numeric, ... ) ) } str_sort(x, numric = TRUE) #&gt; Warning in stringi::stri_opts_collator(locale, numeric = numeric, ...): Unknown #&gt; option to `stri_opts_collator`. #&gt; Error: #&gt; ! Arguments in `...` must be used. #&gt; ✖ Problematic argument: #&gt; • numric = TRUE Note, however, that it’s better to figure out why stri_opts_collator() ignores ... in the first place. You can see that discussion at https://github.com/gagolews/stringi/issues/347. See https://github.com/r-lib/devtools/issues/2016 for discussion about using this in another discussion about using this in devtools::install_github() which is an similar situation, but with a more complicated chain of calls: devtools::install_github() -&gt; install.packages() -&gt; download.file(). "],["cs-mapply-pmap.html", "22 Case study: mapply() vs pmap()", " 22 Case study: mapply() vs pmap() library(purrr) It’s useful to compare mapply() to purrr::pmap(). They both are an attempt to solve a similar problem, extending lapply()/map() to handle iterating over any number of arguments. args(mapply) #&gt; function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) #&gt; NULL args(pmap) #&gt; function (.l, .f, ...) #&gt; NULL x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;) pattern &lt;- c(&quot;p&quot;, &quot;n&quot;, &quot;h&quot;) replacement &lt;- c(&quot;x&quot;, &quot;f&quot;, &quot;q&quot;) mapply(gsub, pattern, replacement, x) #&gt; p n h #&gt; &quot;axxle&quot; &quot;bafafa&quot; &quot;cqerry&quot; mapply(gsub, pattern, replacement, x) #&gt; p n h #&gt; &quot;axxle&quot; &quot;bafafa&quot; &quot;cqerry&quot; purrr::pmap_chr(list(pattern, replacement, x), gsub) #&gt; [1] &quot;axxle&quot; &quot;bafafa&quot; &quot;cqerry&quot; Here we’ll ignore simplify = TRUE which makes mapply() type-unstable by default. I’ll also ignore USE.NAMES = TRUE which isn’t just about using names, but about using character vector input as names for output. I think it’s reused from lapply() without too much thought as it’s only the names of the first argument that matter. mapply(toupper, letters[1:3]) #&gt; a b c #&gt; &quot;A&quot; &quot;B&quot; &quot;C&quot; mapply(toupper, letters[1:3], USE.NAMES = FALSE) #&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; mapply(toupper, setNames(letters[1:3], c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;))) #&gt; X Y Z #&gt; &quot;A&quot; &quot;B&quot; &quot;C&quot; pmap_chr(list(letters[1:3]), toupper) #&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; mapply() takes the function to apply as the first argument, followed by an arbitrary number of arguments to pass to the function. This makes it different to the other apply() functions (including lapply(), sapply() and tapply()), which take the data as the first argument. mapply() could take ... as the first arguments, but that would force FUN to always be named, which would also make it inconsistent with the other apply() functions. pmap() avoids this problem by taking a list of vectors, rather than individual vectors in .... This allows pmap() to use ... for another purpose, instead of the MoreArg argument (a list), pmap() passes ... on to .f. mapply(gsub, pattern, replacement, x, fixed = TRUE) #&gt; p n h #&gt; &quot;axxle&quot; &quot;bafafa&quot; &quot;cqerry&quot; purrr::pmap_chr(list(pattern, replacement, x), gsub, fixed = TRUE) #&gt; [1] &quot;axxle&quot; &quot;bafafa&quot; &quot;cqerry&quot; There’s a subtle difference here that doesn’t matter in most cases - in the mapply() fixed is recycled to the same length as pattern whereas it is not pmap(). TODO: figure out example where that’s more clear. (Also note that pmap() uses the . prefix to avoid the problem described in Chapter 20.) "],["out-multi.html", "23 Returning multiple values 23.1 Different sizes 23.2 Same size 23.3 Case study: str_locate()", " 23 Returning multiple values 23.1 Different sizes Use a list. Name it. If you return the same type of output from multiple functions, you should create a function that consistently creates exact the same format (to avoid accidentally inconsistency), and consider making it an S3 class (so you can have a custom print method). 23.2 Same size When a function returns two vectors of the same size, as a general rule should you return a tibble: A matrix would only work if the vectors were the same type (and not factor or Date), doesn’t make it easy to extract the individual values, and is not easily input to other tidyverse functions. A list doesn’t capture the constraint that both vectors are the same length. A data frame is ok if you don’t want to take a dependency on tibble, but you need to remember the drawbacks: if the columns are character vectors you’ll need to remember to use stringsAsFactors = FALSE, and the print method is confusing for list- and df-cols (and you have to create by modifying an existing data frame, not by calling data.frame()). (Example: it would be weird if glue returned tibbles from a function.) 23.3 Case study: str_locate() e.g. str_locate(), str_locate_all() Interaction with str_sub(). "],["out-type-stability.html", "24 Type-stability 24.1 Simple examples 24.2 More complicated examples 24.3 Challenge: the median 24.4 Exercises", " 24 Type-stability The less you need to know about a function’s inputs to predict the type of its output, the better. Ideally, a function should either always return the same type of thing, or return something that can be trivially computed from its inputs. If a function is type-stable it satisfies two conditions: You can predict the output type based only on the input types (not their values). If the function uses ..., the order of arguments in does not affect the output type. library(vctrs) 24.1 Simple examples purrr::map() and base::lapply() are trivially type-stable because they always return lists. paste() is type stable because it always returns a character vector. vec_ptype(paste(1)) #&gt; character(0) vec_ptype(paste(&quot;x&quot;)) #&gt; character(0) base::mean(x) almost always returns the same type of output as x. For example, the mean of a numeric vector is a numeric vector, and the mean of a date-time is a date-time. vec_ptype(mean(1)) #&gt; numeric(0) vec_ptype(mean(Sys.time())) #&gt; POSIXct of length 0 ifelse() is not type-stable because the output type depends on the value: vec_ptype(ifelse(NA, 1L, 2)) #&gt; &lt;unspecified&gt; [0] vec_ptype(ifelse(FALSE, 1L, 2)) #&gt; numeric(0) vec_ptype(ifelse(TRUE, 1L, 2)) #&gt; integer(0) 24.2 More complicated examples Some functions are more complex because they take multiple input types and have to return a single output type. This includes functions like c() and ifelse(). The rules governing base R functions are idiosyncratic, and each function tends to apply it’s own slightly different set of rules. Tidy functions should use the consistent set of rules provided by the vctrs package. 24.3 Challenge: the median A more challenging example is median(). The median of a vector is a value that (as evenly as possible) splits the vector into a lower half and an upper half. In the absence of ties, mean(x &gt; median(x)) == mean(x &lt;= median(x)) == 0.5. The median is straightforward to compute for odd lengths: you simply order the vector and pick the value in the middle, i.e. sort(x)[(length(x) - 1) / 2]. It’s clear that the type of the output should be the same type as x, and this algorithm can be applied to any vector that can be ordered. But what if the vector has an even length? In this case, there’s no longer a unique median, and by convention we usually take the mean of the middle two numbers. In R, this makes the median() not type-stable: typeof(median(1:3)) #&gt; [1] &quot;integer&quot; typeof(median(1:4)) #&gt; [1] &quot;double&quot; Base R doesn’t appear to follow a consistent principle when computing the median of a vector of length 2. Factors throw an error, but dates do not (even though there’s no date half way between two days that differ by an odd number of days). median(factor(1:2)) #&gt; Error in median.default(factor(1:2)): need numeric data median(Sys.Date() + 0:1) #&gt; [1] &quot;2022-07-20&quot; To be clear, the problems caused by this behaviour are quite small in practice, but it makes the analysis of median() more complex, and it makes it difficult to decide what principle you should adhere to when creating median methods for new vector classes. median(&quot;foo&quot;) #&gt; [1] &quot;foo&quot; median(c(&quot;foo&quot;, &quot;bar&quot;)) #&gt; Warning in mean.default(sort(x, partial = half + 0L:1L)[half + 0L:1L]): argument #&gt; is not numeric or logical: returning NA #&gt; [1] NA 24.4 Exercises How is a date like an integer? Why is this inconsistent? vec_ptype(mean(Sys.Date())) #&gt; Date of length 0 vec_ptype(mean(1L)) #&gt; numeric(0) "],["out-vectorisation.html", "25 Vectorisation", " 25 Vectorisation Vectorisation has two meanings: it can refer to either the interface of a function, or its implementation. We can make a precise statement about what a vectorised interface is. A function, f, is vectorised over a vector argument, x, iff f(x)[[i]] equals f(x[[i]]), i.e. we can exchange the order of subsetting and function application. This generalises naturally to more arguments: we say f is vectorised over x and y if f(x[[i]], y[[i]]) equals f(x, y)[[i]]. A function can have some arguments that are vectorised and some that are not, f(x, ...)[[i]] equals f(x[[i]], ...). It is harder to define vectorised implementation. It’s necessary for a function with a vectorised implementation to have a vectorised interface, but it also must possess the property of computational efficiency. It’s hard to make this precise, but generally it means that if there is an explicit loop, that loop is written in C or C++, not in a R. "],["out-invisible.html", "26 Side-effect functions should return invisibly 26.1 What’s the pattern? 26.2 What are some examples? 26.3 Why is it important?", " 26 Side-effect functions should return invisibly 26.1 What’s the pattern? If a function is called primarily for its side-effects, it should invisibly return a useful output. If there’s no obvious output, return the first argument. This makes it possible to use the function with in a pipeline. 26.2 What are some examples? print(x) invisibly returns the printed object. x &lt;- y invisible returns y. This is what makes it possible to chain together multiple assignments x &lt;- y &lt;- z &lt;- 1 readr::write_csv() invisibly returns the data frame that was saved. purrr::walk() invisibly returns the vector iterated over. fs:file_copy(from, to) returns to options() and par() invisibly return the previous value so you can reset with on.exit(). 26.3 Why is it important? Invisibly returning the first argument allows to call the function mid-pipe for its side-effects while allow the primary data to continue flowing through the pipe. This is useful for generating intermediate diagnostics, or for saving multiple output formats. library(dplyr, warn.conflicts = FALSE) library(tibble) mtcars %&gt;% as_tibble() %&gt;% filter(cyl == 6) %&gt;% print() %&gt;% group_by(vs) %&gt;% summarise(mpg = mean(mpg)) #&gt; # A tibble: 7 × 11 #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 #&gt; 3 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 #&gt; 4 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; 5 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 #&gt; 6 17.8 6 168. 123 3.92 3.44 18.9 1 0 4 4 #&gt; 7 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6 #&gt; # A tibble: 2 × 2 #&gt; vs mpg #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 20.6 #&gt; 2 1 19.1 library(readr) mtcars %&gt;% write_csv(&quot;mtcars.csv&quot;) %&gt;% write_tsv(&quot;mtcars.tsv&quot;) unlink(c(&quot;mtcars.csv&quot;, &quot;mtcars.tsv&quot;)) library(fs) paths &lt;- file_temp() %&gt;% dir_create() %&gt;% path(letters[1:5]) %&gt;% file_create() paths #&gt; /tmp/RtmpDPqf91/file30a81409f144/a /tmp/RtmpDPqf91/file30a81409f144/b #&gt; /tmp/RtmpDPqf91/file30a81409f144/c /tmp/RtmpDPqf91/file30a81409f144/d #&gt; /tmp/RtmpDPqf91/file30a81409f144/e Functions that modify some global state, like options() or par(), should return the previous value of the variables. This, in combination with Section 10.3.2, makes it possible to easily reset the effect of the change: x &lt;- runif(1) old &lt;- options(digits = 3) x #&gt; [1] 0.394 options(old) x #&gt; [1] 0.3938863 "],["err-call.html", "27 Error call", " 27 Error call Don’t display the call when generating an error message. Either use stop(call. = FALSE) or rlang::abort() to avoid it. Why not? Typically doesn’t display enough information to find the source of the call (since most errors are not from top-level function calls), and you can expect the most people to either use RStudio, or know how to call traceback(). "],["err-constructor.html", "28 Error constructors 28.1 What’s the pattern? 28.2 Why is this important? 28.3 What does an error constructor do? 28.4 How do I test? 28.5 Error hierarchies", " 28 Error constructors 28.1 What’s the pattern? Following the rule of three, whenever you generate the same error in three or more places, you should extract it out into a common function, called an error constructor. This function should create a custom condition that contains components that can easily be tested and a conditionMessage() method that generates user friendly error messages. (This is a new pattern that we are currently rolling out across the tidyverse; it’s currently found in few packages.) library(rlang) 28.2 Why is this important? If you don’t use an custom condition, you can only check that your function has generated the correct error by matching the text of the error message with a regular expression. This is fragile because the text of error messages changes relatively frequently, causing spurious test failures. You can use custom conditions for one-off errors, but generally the extra implementation work is not worth the pay off. That’s why we recommend only using an error constructor for repeated errors. It gives more precise control over error handling with tryCatch(). This is particularly useful in packages because you may be able to give more useful high-level error mesasges by wrapping a specific low-level error. As you start using this technique for more error messages you can create a hierarchy of errors that allows you to borrow behaviour, reducing the amount of code you need to write. Once you have identified all the errors that can be thrown by a function, you can add a @section Throws: to the documentation that precisely describes the possible failure modes. 28.3 What does an error constructor do? An error constructor is very similar to an S3 constructor, as its job is to extract out repeated code and generate a rich object that can easily be computed with. The primary difference is that instead of creating and returning a new object, it creates a custom error and immediately throws it with abort(). Here’s a simple imaginary error that might be thrown by fs if it couldn’t find a file: stop_not_found &lt;- function(path) { abort( .subclass = &quot;fs_error_not_found&quot;, path = path ) } Note the naming scheme: The function should be called stop_{error_type} The error class should be {package}_error_{error_type}. The function should have one argument for each varying part of the error, and these argument should be passed onto abort() to be stored in the condition object. To generate the error message shown to the user, provide a conditionMessage() method: #&#39; @export conditionMessage.fs_error_not_found &lt;- function(c) { glue::glue_data(c, &quot;&#39;{path}&#39; not found&quot;) } stop_not_found(&quot;a.csv&quot;) #&gt; Error: &#39;a.csv&#39; not found This method must be exported, because you are defining a method for a generic in another package, and it will often use glue::glue_data() to assemble the components of the condition into a string. See https://style.tidyverse.org/error-messages.html for advice on writing the error message. 28.4 How do I test? library(testthat) #&gt; #&gt; Attaching package: &#39;testthat&#39; #&gt; The following objects are masked from &#39;package:rlang&#39;: #&gt; #&gt; is_false, is_null, is_true 28.4.1 Test the constructor Firstly, you should test the error constructor. The primary goal of this test is to ensure that the error constructor generates a message that is useful to humans, which you can not automate. This means that you can not use a unit test (because the desired output is not known) and instead you need to use a regression test, so you can ensure that the message does not change unexpectedly. For that reason the best approach is usually to use verify_output(), e.g.: test_that(&quot;stop_not_found() generates useful error message&quot;, { verify_output(test_path(&quot;test-stop-not-found.txt&quot;), { stop_not_found(&quot;a.csv&quot;) }) }) This is useful for pull requests because verify_output() generates a complete error messages in a text file that can easily be read and reviewed. If your error has multiple arguments, or your conditionMessage() method contains if statements, you should generally attempt to cover them all in a test case. 28.4.2 Test usage Now that you have an error constructor, you’ll need to slightly change how you test your functions that use the error constructor. For example, take this imaginary example for reading a file into a single string: read_lines &lt;- function(x) { if (!file.exists(x)) { stop_not_found(x) } paste0(readLines(x), collapse = &quot;\\n&quot;) } Previously, you might have written: expect_error(read_lines(&quot;missing-file.txt&quot;), &quot;not found&quot;) But, now as you see, testthat gives you a warning that suggests you need to use the class argument instead: expect_error(read_lines(&quot;missing-file.txt&quot;), class = &quot;fs_error_not_found&quot;) This is less fragile because you can now change the error message without having to worry about breaking existing tests. If you also want to check components of the error object, note that expect_error() returns it: cnd &lt;- expect_error(read_lines(&quot;missing-file.txt&quot;), class = &quot;fs_error_not_found&quot;) expect_equal(cnd$path, &quot;missing-file.txt&quot;) I don’t think this level of testing is generally important, so you should only use it because the error generation code is complex conditions, or you have identified a bug. 28.5 Error hierarchies As you start writing more and more error constructors, you may notice that you are starting to share code between them because the errors form a natural hierarchy. To take advantage of this hierarchy to reduce the amount of code you need to write, you can make the errors subclassable by adding ... and class arguments: stop_not_found &lt;- function(path, ..., class = character()) { abort( .subclass = c(class, &quot;fs_error_not_found&quot;), path = path ) } Then the subclasses can call this constructor, and the problem becomes one of S3 class design. We currently have little experience with this, so use with caution. "],["changes-multivers.html", "29 Work with multiple dependency versions 29.1 What’s the pattern? 29.2 Writing code 29.3 Testing with multiple package versions 29.4 Using only the new version", " 29 Work with multiple dependency versions 29.1 What’s the pattern? In an ideal world, when a dependency of your package changes its interface, you want your package to work with both versions. This is more work but it has two significant advantages: The CRAN submission process is decoupled. If your package only works with the development version of a dependency, you’ll need to carefully coordinate your CRAN submission with the dependencies CRAN submission. If your package works with both versions, you can submit first, making life easier for CRAN and for the maintainer of the dependency. User code is less likely to be affected. If your package only works with the latest version of the dependency, then when a user upgrades your package, the dependency also must update. Upgrading multiple packages is more likely to affect user code than updating a single package. In this pattern, you’ll learn how to write code designed to work with multiple versions of a dependency, and you’ll how to adapt your existing Travis configuration to test that you’ve got it right. 29.2 Writing code Sometimes there will be an easy way to change the code to work with both old and new versions of the package; do this if you can! However, in most cases, you can’t, and you’ll need an if statement that runs different code for new and old versions of the package: if (dependency_has_new_interface()) { # freshly written code that works with in-development dependency } else { # existing code that works with the currently released dependency } (If your freshly written code uses functions that don’t exist in the CRAN version this will generate an R CMD check NOTE when you submit it to CRAN. This is one of the few NOTEs that you can explain: just mention that it’s needed for forward/backward compatibility in your submission notes.) We recommend always pulling out the check out into a function so that the logic lives in one place. This will make it much easier to pull it out when it’s no longer needed, and provides a good place to document why it’s needed. There are three basic approaches to implement dependency_has_new_interface(): Check the version of the package. This is recommended in most cases, but requires that the dependency author use a specific version convention. Check for existence of a function. Check for a specific argument value, or otherwise detect that the interface has changed. 29.2.1 Case study: tidyr To make the problem concrete so we can show of some real code, lets imagine we have a package that uses tidyr::nest(). tidyr::nest() changed substantially between 0.8.3 and 1.0.0, and so we need to write code like this: if (tidyr_new_interface()) { out &lt;- tidyr::nest_legacy(df, x, y, z) } else { out &lt;- tidyr::nest(df, c(x, y, z)) } (As described above, when submitted to CRAN this will generate a note about missing tidyr::nest_legacy() which can be explained in the submission comments.) To implement tidyr_new_interface(), we need to think about three versions of tidyr: 0.8.3: the version currently on CRAN with the old interface. 0.8.99.9000: the development version with the new interface. As usualy, the fourth component is &gt;= 9000 to indicate that it’s a development version. Note, however, that the patch version is 99; this indicates that release includes breaking changes. 1.0.0: the future CRAN version; this is the version that will be submitted to CRAN. The main question is how to write tidyr_new_interface(). There are three options: Check that the version is greater than the development version: tidyr_new_interface &lt;- function() { packageVersion(&quot;tidyr&quot;) &gt; &quot;0.8.99&quot; } This technique works because tidyr uses the convention that the development version of backward incompatible functions contain 99 in the third (patch) component. If tidyr didn’t adopt this naming convention, we could test for the existence of unnest_legacy(). tidyr_new_interface1 &lt;- function() { exists(&quot;unnest_legacy&quot;, asNamespace(&quot;tidyr&quot;)) } If the inteface change was more subtle, you might have to think more creatively. If the package uses the lifecycle system, one approach would be to test for the presence of deprecated() in the function arguments: tidyr_new_interface2 &lt;- function() { identical(formals(tidyr::nest)$.key, quote(deprecated())) } All these approaches are reaosnably fast, so it’s unlikely they’ll have any impact on performance unless called in a very tight loop. bench::mark( version = tidyr_new_interface(), exists = tidyr_new_interface1(), formals = tidyr_new_interface2() )[1:5] #&gt; # A tibble: 3 × 5 #&gt; expression min median `itr/sec` mem_alloc #&gt; &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;dbl&gt; &lt;bch:byt&gt; #&gt; 1 version 713.9µs 813.5µs 1222. 3.82KB #&gt; 2 exists 3.4µs 4µs 226810. 17.09MB #&gt; 3 formals 1.4µs 1.9µs 438038. 0B If you do need to use packageVersion() inside a performance sensitive function, I recommend caching the result in .onLoad() (which, by convention, lives in zzz.R). There a few ways to do this; but the following block shows one approach that matches the function interface I used above: tidyr_new_interface &lt;- function() FALSE .onLoad &lt;- function(...) { if (utils::packageVersion(&quot;tidyr&quot;) &gt; &quot;0.8.2&quot;) { tidyr_new_interface &lt;&lt;- function() TRUE } } 29.3 Testing with multiple package versions It’s good practice to test both old and new versions of the code, but this is challenging because you can’t both sets of tests in the same R session. The easiest way to make sure that both versions are work and stay working is to use Travis. Before the dependency is released, you can manually install the development version using remotes::install_github(): matrix: include: - r: release name: tidyr-devel before_script: Rscript -e &quot;remotes::install_github(&#39;tidyverse/tidyr&#39;)&quot; It’s not generally that important to check that your code continues to work with an older version of the package, but if you want to you can use remotes::install_version(): matrix: include: - r: release name: tidyr-0.8 before_script: Rscript -e &quot;remotes::install_version(&#39;tidyr&#39;, &#39;0.8.3&#39;)&quot; 29.4 Using only the new version At some point in the future, you’ll decide that the old version of the package is no longer widely used and you want to simplify your package by only depending on the new version. There are three steps: In the DESCRIPTION, bump the required version of the dependency. Search for dependency_has_new_interface(); remove the function defintion and all uses (retaining the code used with the new version). Remove the additional build in .travis.yml. "],["side-effect-soup.html", "30 Side-effect soup 30.1 What is a side-effect? 30.2 What are some examples? 30.3 Why is it bad? 30.4 How avoid it? 30.5 Package considerations", " 30 Side-effect soup Side-effect soup occurs when you mix side-effects and regular computation within the same function. 30.1 What is a side-effect? There are two main types of side-effect: those that give feedback to the user. those that change some global state. 30.1.1 User feedback Signalling a condition, with message(), warning(), or stop(). Printing to the console with cat(). Drawing to the current graphics device with base graphics or grid. 30.1.2 Global state Creating (or modifying) an existing binding with &lt;-. Modifying the search path by attaching a package with library(). Changing the working directory with setwd(). Modifying a file on disk with (e.g.) write.csv(). Changing a global option with options() or a base graphics parameter with gpar(). Setting the random seed with set.seed() Installing a package. Changing environment variables with Sys.setenv(), or indirectly via a function like Sys.setlocale(). Modifying a variable in an enclosing environment with assign() or &lt;&lt;-. Modifying an object with reference semantics (like R6 or data.table). More esoteric side-effects include: Detaching a package from the search path with detach(). Changing the library path, where R looks for packages, with .libPaths() Changing the active graphics device with (e.g.) png() or dev.off(). Registering an S4 class, method, or generic with methods::setGeneric(). Modifying the internal .Random.seed 30.2 What are some examples? The summary of a linear model includes a p-value for the overall regression. This value is only computed when the summary is printed: you can see it but you can’t touch it. mod &lt;- lm(mpg ~ wt, data = mtcars) summary(mod) #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = mtcars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.5432 -2.3647 -0.1252 1.4096 6.8727 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** #&gt; wt -5.3445 0.5591 -9.559 1.29e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.046 on 30 degrees of freedom #&gt; Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 #&gt; F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 30.3 Why is it bad? Side-effect soup is bad because: If a function does some computation and has side-effects, it can be challenging to extract the results of computation. Makes code harder to analyse because it may have non-local effects. Take this code: x &lt;- 1 y &lt;- compute(x) z &lt;- calculate(x, y) df &lt;- data.frame(x = &quot;x&quot;) If compute() or calculate() don’t have side-effects then you can predict what df will be. But if compute() did options(stringsAsFactors = FALSE) then df would now contain a character vector rather than a factor. Side-effect soup increases the cognitive load of a function so should be used deliberately, and you should be especially cautious when combining them with other techniques that increase cognitive load like tidy-evaluation and type-instability. 30.4 How avoid it? 30.4.1 Localise side-effects Constrain the side-effects to as small a scope as possible, and clean up automatically to avoid side-effects. withr 30.4.2 Extract side-effects It’s not side-effects that are bad, so much as mixing them with non-side-effect code. Put them in a function that is specifically focussed on the side-effect. If your function is called primarily for its side-effects, it should return the primary data structure (which should be first argument), invisibly. This allows you to call it mid-pipe for its side-effects while allow the primary data to continue flowing through the pipe. 30.4.3 Make side-effects noisy Primary purpose of the entire package is side-effects: modifying files on disk to support package and project development. usethis functions are also designed to be noisy: as well as doing it’s job, each usethis function tells you what it’s doing. But some usethis functions are building blocks for other more complex tasks. 30.4.4 Provide an argument to suppress You’ve probably used base::hist() for it’s side-effect of drawing a histogram: x &lt;- rnorm(1e5) hist(x) But you might not know that hist() also returns the result of the computation. If you call plot = FALSE it will simply return the results of the computation: xhist &lt;- hist(x, plot = FALSE) str(xhist) #&gt; List of 6 #&gt; $ breaks : num [1:19] -4.5 -4 -3.5 -3 -2.5 -2 -1.5 -1 -0.5 0 ... #&gt; $ counts : int [1:18] 3 23 97 486 1614 4486 9149 14807 19099 19276 ... #&gt; $ density : num [1:18] 0.00006 0.00046 0.00194 0.00972 0.03228 ... #&gt; $ mids : num [1:18] -4.25 -3.75 -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 ... #&gt; $ xname : chr &quot;x&quot; #&gt; $ equidist: logi TRUE #&gt; - attr(*, &quot;class&quot;)= chr &quot;histogram&quot; This is a good approach for retro-fitting older functions while making minimal API changes. However, I think it dilutes a function to be both used for plotting and for computing so should be best avoided in newer code. 30.4.5 Use the print() method An alternative approach would be to always return the computation, and instead perform the output in the print() method. Of course ggplot2 isn’t perfect: it creates an object that specifies the plot, but there’s no easy way to extract the underlying computation so if you’ve used geom_smooth() to add lines of best fit, there’s no way to extract the values. Again, you can see the results, but you can’t touch them, which is very frustrating! 30.4.6 Make easy to undo If all of the above techniques fail, you should at least make the side-effect easy to undo. A use technique to do this is to make sure that the function returns the previous values, and that it can take it’s own input. This is how options() and par() work. You obviously can’t eliminate those functions because their complete purpose is have global changes! But they are designed in such away that you can easily undo their operation, making it possible to apply on a local basis. There are two key ideas that make these functions easy to undo: They invisibly return the previous values as a list: options(my_option = 1) old &lt;- options(my_option = 2) str(old) #&gt; List of 1 #&gt; $ my_option: num 1 Instead of n named arguments, they can take a single named list: old &lt;- options(list(my_option1 = 1, my_option2 = 2)) (I wouldn’t recommend copying this technique, but I’d instead recommend always taking a single named list. This makes the function because it has a single way to call it and makes it easy to extend the API in the future, as discussed in Chapter 19) Together, this means that you easily can set options temporarily.: getOption(&quot;my_option1&quot;) #&gt; [1] 1 old &lt;- options(my_option1 = 10) getOption(&quot;my_option1&quot;) #&gt; [1] 10 options(old) getOption(&quot;my_option1&quot;) #&gt; [1] 1 If temporarily setting options in a function, you should always restore the previous values using on.exit(): this ensures that the code is run regardless of how the function exits. 30.5 Package considerations Code in package is executed at build-time.i.e. if you have: x &lt;- Sys.time() For mac and windows, this will record when CRAN built the binary. For linux, when the package was installed. Beware copying functions from other packages: foofy &lt;- barfy::foofy Version of barfy might be different between run-time and build-time. Introduces a build-time dependency. https://github.com/r-lib/devtools/issues/1788 "],["spooky-action.html", "31 Spooky action 31.1 What’s the problem? 31.2 What precisely is a spooky action? 31.3 How can I remediate spooky actions? 31.4 Case studies", " 31 Spooky action 31.1 What’s the problem? There are no limits to what an function or script can do. After you call draw_plot() or source(\"analyse-data.R\"), you could discover that all the variables in your global environment have been deleted, or that 1000 new files have been created on your desktop. But these actions would be surprising, because generally you expect the impact of a function (or script) to be as limited as possible. Collectively, we call such side-effects “spooky actions” because the connection between action (calling a function or sourcing a script) and result (deleting objects or upgrading packages) is surprising. It’s like flipping a light-switch and discovering that the shower starts running, or having a poltergeist that rearranges the contents of your kitchen cupboards when you’re not looking. Deleting variables and creating files on your desktop are obviously surprising even if you’ve only just started using R. But there are other actions that are less obviously destructive, and only start to become surprising as your mental model of R matures. These include actions like: Attaching packages with library(). For example, ggplot2::geom_map() used to call library(maps) in order to make map data available to the function. This seems harmless, but if you were using purrr, it would break map() map() would now refer to maps::map() rather than purrr::map(). Because of functions in different packages can have the same name, attaching a package can change the behaviour of existing code. Installing packages with install.packages(). If a script needs dplyr to work, and it’s not installed, it seem polite to install it on behalf of the user. But installing a new package can upgrade existing packages, which might break code in other projects. Install a package is a potentially destructive operation which should be done with care. Deleting objects in the global environment with rm(list = ls()). This might seem like a good way to reset the environment so that your script can run cleanly. But if someone else source()s your script, it will delete objects that might be important to them. (Of course, you’d hope that all of those objects could easily be recreated from another script, but that is not always the case). Because R doesn’t constrain the potential scope of functions and scripts, you have to. By avoiding these actions, you will create code that is less surprising to other R users. At first, this might seem like tedious busywork. You might find that spooky action is convenient in the moment, and you might convince yourself that it’s necessary or a good idea. But as you share your code with more people and run more code that has been shared with you5, you’ll find spooky action to get more and more surprising and frustrating. 31.2 What precisely is a spooky action? We can make the notion of spooky action precise by thinking about trees. Code should only affect the tree beneath where it lives, so any action that reaches up, or across, the tree is a spooky action. There are two important types of trees to consider: The tree formed by files and directories. A script should only read from and write to directories beneath the directory where it lives. This explains why you shouldn’t install packages (because the package library usually lives elsewhere), and also explains why you shouldn’t create files on the desktop. This rule can be relaxed in two small ways. Firstly, if the script lives in a project, it’s ok to read from and write to anywhere in the project (i.e. a file in R/ can read from data-raw/ and write to data/). Secondly, it’s always ok to write to the session specific temporary directory, tempdir(). This directory is automatically deleted when R closes, so does not have any lasting effects. The tree of environments created by function calls. A function should only create and modify variables in its own environment or environments that it creates (typically by calling other functions). This explains why you shouldn’t attach packages (because that changes the search path), why you shouldn’t delete variables with rm(list = ls()), or assign to variables that you didn’t create with &lt;&lt;-. 31.3 How can I remediate spooky actions? If you have read the above cautions, and still want to proceed, there are three ways you can make the spooky action as safe as possible: Allow the user to control the scope. Make the action less spooky by giving it a name that clearly describes what it will do. Explicitly check with the user before proceeding with the action. Advertise what’s happening, so while the action might still be spooky, at least it isn’t surprising. 31.3.1 Parameterise the action The first technique is to allow the user to control where the action will occur. For example, instead of save_output_desktop(), you would write save_output(path), and require that the user provide the path. 31.3.2 Advertise the action with a clear name If you can’t parameterise the action, make it clear what’s going to happen from the outside. It is fine for function or scripts to have actions outside of their usual trees as long as it is implicit in the name: It’s ok for &lt;- to modify the global environment, because that is its one job, and it’s obvious from the name (once you’ve learned about &lt;-, which happens very early). Similarly, it’s ok for save_output_to_desktop() to create files in on the desktop, or copy_to_clipboard() to copy text to the clipboard, because the action is clear from the name. It’s fine for install.packages() to modify files outside of the current working directory because it’s designed specifically to install packages. Similarly, it’s ok for source(\"class-setup.R\") to install packages because the intent of a setup script is to get your computer into the same state as someone else’s. Here, it’s the name of the function or script that is really important. As soon as you Note that it’s the name that’s important - it’s fine for install.packages() to install packages, but it’s not ok as soon as it’s hidden behind even a very simple wrapper: current_time &lt;- function() { if (!requireNamespace(&quot;lubridate&quot;, quietly = TRUE)) { install.packages(&quot;lubridate&quot;) } lubridate::now() } current_time() #&gt; [1] &quot;2022-07-20 18:18:53 UTC&quot; 31.3.3 Ask for confirmation If you can’t parameterise the operation, and need to perform it from somewhere deep within the cope, make sure to confirm with the user before performing the action. The code below shows how you might do so when installing a package: install_if_needed &lt;- function(package) { if (requireNamespace(package, quietly = TRUE)) { return(invisible(TRUE)) } if (!interactive()) { stop(package, &quot; is not installed&quot;, call. = FALSE) } title &lt;- paste0(package, &quot; is not installed. Do you wish to install now?&quot;) if (menu(c(&quot;Yes&quot;, &quot;No&quot;), title = title) != 1) { stop(&quot;Confirmation not received&quot;, call. = FALSE) } invisible(TRUE) } Note the use of interactive() here: if the user is not in an interactive setting (i.e. the code is being run with Rscript) and we can not get explicit confirmation, we shouldn’t make any changes. Also that all failures are errors: this ensures that the remainder of the function or script does not run if the user doesn’t confirm. Ideally this function would also clearly describe the consequences of your decision. For example, it would be nice to know if it will download a significant amount of data (since you might want to wait until your at a fast connection if downloading a 1 Gb data package), or if it will upgrade existing packages (since that might break other code). Writing code that checks with the user requires some care, and it’s easy to get the details wrong. That’s why it’s better to prefer one of the prior techniques. 31.3.4 Advertise the side-effects If you can’t get explicit confirmation from the user, at the very minimum you should clearly advertise what is happening. For example, when you call install.packages() it notifies you: install.packages(&quot;dplyr&quot;) #&gt; Installing package into ‘/Users/hadley/R’ #&gt; (as ‘lib’ is unspecified) #&gt; Trying URL &#39;https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.5/dplyr_0.8.0.1.tgz&#39; #&gt; Content type &#39;application/x-gzip&#39; length 6587031 bytes (6.3 MB) #&gt; ================================================== #&gt; downloaded 6.3 MB However, this message could do with some work: It says installing “package”, without specifying which package (so if this is called inside another function it won’t be informative). It doesn’t notify me which dependencies it’s also going to update. It notifies me of the url it’s downloading from, which I don’t care about, and it only notifies me about the size when it’s finished downloading, by which time it too late to stop it. I would instead write something like this: install.packages(&quot;dplyr&quot;) #&gt; Installing package dplyr to `/Users/hadley/R` #&gt; Also installing 3 dependencies: glue, rlang, R6 We’ll come back to the issue of informing the user in … 31.4 Case studies 31.4.1 save() and load() load() has a spooky action because it modifies variables in the current environment: x &lt;- 1 load(&quot;spooky-action.rds&quot;) x #&gt; [1] 10 You can make it less spooky by supplying verbose = TRUE. Here we learn that it also loaded a y object: load(&quot;spooky-action.rds&quot;, verbose = TRUE) #&gt; Loading objects: #&gt; x #&gt; y y #&gt; [1] 100 (In an ideal world verbose = TRUE would be default) But generally, I’d avoid save() and load() altogether, and instead use saveRDS() and readRDS(), which read and write individual R objects to individual R files and work with &lt;-. This eliminates all spooky action: saveRDS(x, &quot;x.rds&quot;) x &lt;- readRDS(&quot;x.rds&quot;) unlink(&quot;x.rds&quot;) (readr provides readr::read_rds() and readr::write_rds() if the inconsistent naming conventions bother you like they bother me.) 31.4.2 usethis The usethis package is designed to support the process of developing a package of R code. It automates many of tedious setup steps by providing function like use_r() or use_test(). Many usethis functions modify the DESCRIPTION and create other files. usethis makes these actions as pedestrian as possible by: Making it clear that the entire package is designed for the purpose of creating and modifying files, and the purpose of each function is clearly encoded in its named. For any potential risky operation, e.g. overwriting an existing file, usethis explicitly asks for confirmation from the user. To make it harder to “click” through prompts without reading them, usethis uses random prompts in a random ordering. usethis::ui_yeah(&quot;Do you want to proceed?&quot;) #&gt; Do you want to proceed? #&gt; #&gt; 1: Absolutely not #&gt; 2: Not now #&gt; 3: I agree usethis also works in concert with git to make sure that change are captured in a way that can easily be undone. When you call it, every usethis function describes what it is doing as it it doing it: usethis::create_package(&quot;mypackage&quot;, open = FALSE) #&gt; ✔ Creating &#39;mypackage&#39; #&gt; ✔ Setting active project to &#39;mypackage&#39; #&gt; ✔ Creating &#39;R/&#39; #&gt; ✔ Writing &#39;DESCRIPTION&#39; #&gt; ✔ Writing &#39;NAMESPACE&#39; #&gt; ✔ Writing &#39;mypackage.Rproj&#39; #&gt; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; #&gt; ✔ Adding &#39;^mypackage\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; This is important, but it’s not clear how impactful it is because many functions produce enough output that reading through it all seems onerous and so generally most people don’t read it. 31.4.3 &lt;&lt;- If you haven’t heard of &lt;&lt;-, the super-assignment operator, before, feel free to skip this section as it’s an advanced technique that has relatively limited applications. They’re most important in the context of functionals, which you can read more about in Advanced R. &lt;&lt;- is safe if you use it to modify a variable in an environment that you control. For example, the following code creates a function that counts the number of times it is called. The use of &lt;&lt;- is safe because it only affects the environment created by make_counter(), not an external environment. make_counter &lt;- function() { i &lt;- 0 function() { i &lt;&lt;- i + 1 i } } c1 &lt;- make_counter() c2 &lt;- make_counter() c1() #&gt; [1] 1 c1() #&gt; [1] 2 c2() #&gt; [1] 1 A more common use of &lt;&lt;- is to break one of the limitations of map()6 and use it like a for loop to iteratively modify input. For example, imagine you want to compute a cumulative sum. That’s straightforward to write with a for loop: x &lt;- rpois(10, 10) out &lt;- numeric(length(x)) for (i in seq_along(x)) { if (i == 1) { out[[i]] &lt;- x[[i]] } else { out[[i]] &lt;- x[[i]] + out[[i - 1]] } } rbind(x, out) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; x 8 8 11 5 12 8 10 12 9 12 #&gt; out 8 16 27 32 44 52 62 74 83 95 A simple transformation from to use map() doesn’t work: library(purrr) out &lt;- numeric(length(x)) map_dbl(seq_along(x), function(i) { if (i == 1) { out[[i]] &lt;- x[[i]] } else { out[[i]] &lt;- x[[i]] + out[[i - 1]] } }) #&gt; [1] 8 8 11 5 12 8 10 12 9 12 Because the modification of out is happening inside of a function, R creates a copy of out (this is called copy-on-modify principle). Instead we need to use &lt;&lt;- to reach outside of the function to modify the outer out: map_dbl(seq_along(x), function(i) { if (i == 1) { out[[i]] &lt;&lt;- x[[i]] } else { out[[i]] &lt;&lt;- x[[i]] + out[[i - 1]] } }) #&gt; [1] 8 16 27 32 44 52 62 74 83 95 This use of &lt;&lt;- is a spooky action because we’re reaching up the tree of environments to modify an object created outside of the function. In this case, however, there’s no point in using map(): the point of those functions is to restrict what you can do compared to a for loop so that your code is easier to understand. R for data science has other examples of for loops that could be rewritten with map(), but shouldn’t be. Note that we could still wrap this code into a function to eliminate the spooky action: cumsum2 &lt;- function(x) { out &lt;- numeric(length(x)) map_dbl(seq_along(x), function(i) { if (i == 1) { out[[i]] &lt;&lt;- x[[i]] } else { out[[i]] &lt;&lt;- x[[i]] + out[[i - 1]] } }) out } This eliminates the spooky action because it’s now modifying an object that the function “owns”, but I still wouldn’t recommend it, as the use of map() and &lt;&lt;- only increases complexity for no gain compared to the use of a for loop. 31.4.4 assign() in a for loop It’s not uncommon for people to ask how to create multiple objects from for a loop. For example, maybe you have a vector of file names, and want to read each file into an individual object. With some effort you typically discover assign(): paths &lt;- c(&quot;a.csv&quot;, &quot;b.csv&quot;, &quot;c.csv&quot;) names(paths) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) for (i in seq_along(paths)) { assign(names(paths)[[i]], read.csv(paths[[i]])) } The main problem with this approach is that it does not facilitate composition. For example, imagine that you now wanted to figure out how many rows are in each of the data frames. Now you need to learn how to loop over a series of objects, where the name of the object is stored in a character vector. With some work, you might discover get() and write: lengths &lt;- numeric(length(names)) for (i in seq_along(paths)) { lengths[[i]] &lt;- nrow(get(names(paths)[[i]])) } This approach is not necessarily bad in and of itself7, but it tends to lead you down a high-friction path. Instead, if you learn a little about lists and functional programming techniques (e.g. with purrr), you’ll be able to write code like this: library(purrr) files &lt;- map(paths, read.csv) lengths &lt;- map_int(files, nrow) This obviously requires that you learn some new tools - but learning about map() and map_int() will pay off in many more situations than learning about assign() and get(). And because you can reuse map() and friends in many places, you’ll find that they get easier and easier to use over time. It would certainly be possible to build tools in purrr to avoid having to learn about assign() and get() and to provide a polished interface for working with character vectors containing object names. But such functions would need to reach up the tree of environments, so would violate the “spooky action” principle, and thus I believe are best avoided. Spooky actions tend to be particularly frustrating to those who teach R, because they have to run scripts from many students, and those scripts can end up doing wacky things to their computers.↩︎ purrr::map() is basically interchangeable with base::lapply() so if you’re more familiar with lapply(), you can mentally substitute it for map() in all the code here.↩︎ However, if you take this approach in multiple places in your code, you’ll need to make sure that you don’t use the same name in multiple loops because assign() will silently overwrite an existing variable. This might not happen commonly but because it’s silent, it will create a bug that is hard to detect.↩︎ "],["glossary.html", "A Glossary", " A Glossary "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
